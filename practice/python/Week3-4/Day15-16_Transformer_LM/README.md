# Day 15-16: PyTorchåŸºç¡€ä¸Transformerè¯­è¨€æ¨¡å‹

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡Transformer LMå®ç°ï¼Œç†è§£BPE Tokenizerï¼Œå®ŒæˆCS336 Assignment 1æ ¸å¿ƒå†…å®¹
> **æ—¶é—´åˆ†é…**: 6å°æ—¶ï¼ˆç†è®º2h + å®è·µ4hï¼‰
> **éš¾åº¦**: â­â­â­â­
> **æ¥æº**: CS336 Assignment 1 - Basics

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. Transformerè¯­è¨€æ¨¡å‹æ¶æ„

**GPT-style Decoder-only Transformer**:
```
è¾“å…¥æ–‡æœ¬ â†’ Tokenizer â†’ Token IDs
         â†“
    Input Embedding (vocab_size Ã— d_model)
         â†“
    Positional Encoding (RoPE)
         â†“
    N Ã— Transformer Blocks:
      - Layer Normalization (RMSNorm)
      - Multi-Head Self-Attention
      - Feed-Forward Network (GELU)
      - Residual Connections
         â†“
    Output Layer Norm
         â†“
    Linear Projection to Vocab
         â†“
    Softmax â†’ Token Probabilities
```

### 2. BPE TokenizeråŸç†

**Byte-Level Byte-Pair Encoding**:

1. **åˆå§‹åŒ–**: å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå­—èŠ‚åºåˆ—ï¼ˆ256ä¸ªåŸºæœ¬tokenï¼‰
2. **è¿­ä»£åˆå¹¶**: ç»Ÿè®¡ç›¸é‚»å­—èŠ‚å¯¹é¢‘ç‡ï¼Œåˆå¹¶æœ€é«˜é¢‘å¯¹
3. **æ„å»ºè¯è¡¨**: é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯è¡¨å¤§å°ï¼ˆ32Kï¼‰
4. **ç¼–ç **: ä½¿ç”¨å­¦åˆ°çš„åˆå¹¶è§„åˆ™ç¼–ç æ–°æ–‡æœ¬

**ä¸ºä»€ä¹ˆä½¿ç”¨Byte-Level BPEï¼Ÿ**
- âœ… æ— éœ€UNKNOWN token
- âœ… å¯å¤„ç†ä»»æ„Unicodeå­—ç¬¦
- âœ… å‹ç¼©ç‡é«˜ï¼ˆç›¸æ¯”å­—ç¬¦çº§ï¼‰
- âœ… é€‚åˆå¤šè¯­è¨€æ–‡æœ¬

### 3. å…³é”®ç»„ä»¶è¯¦è§£

#### 3.1 RMSNormï¼ˆRoot Mean Square Layer Normalizationï¼‰

```python
# æ ‡å‡†LayerNorm vs RMSNorm
# LayerNorm: (x - mean) / std * Î³ + Î²
# RMSNorm: x / RMS(x) * Î³  (æ›´ç®€å•ï¼Œæ— bias)

RMS(x) = sqrt(mean(xÂ² + Îµ))
output = x / RMS(x) * weight
```

#### 3.2 RoPEï¼ˆRotary Positional Encodingï¼‰

**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡æ—‹è½¬çŸ©é˜µæ³¨å…¥ä½ç½®ä¿¡æ¯åˆ°Queryå’ŒKey

```python
# æ—‹è½¬è§’åº¦
Î¸ = 10000^(-2i/d)  # iä¸ºç»´åº¦ç´¢å¼•

# æ—‹è½¬çŸ©é˜µ
m: ä½ç½®ç´¢å¼•
rot(m, i) = exp(m * Î¸ * i)

# åº”ç”¨åˆ°Qå’ŒK
q_rotated = q * cos(mÎ¸) + rotate(q) * sin(mÎ¸)
k_rotated = k * cos(mÎ¸) + rotate(k) * sin(mÎ¸)
```

#### 3.3 Multi-Head Attention

**æ ‡å‡†å…¬å¼**:
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

**Multi-Head**:
```
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O
```

---

## ğŸ”§ CS336 Assignment 1 è¦æ±‚è¯¦è§£

### ä»»åŠ¡æ¸…å•

#### Part 1: BPE Tokenizer (2å°æ—¶)

**æ–‡ä»¶**: `cs336_basics/data.py`

**éœ€è¦å®ç°çš„å‡½æ•°**:

1. **`train_bpe(training_data, vocab_size)`** (1.5h)
   ```python
   def train_bpe(
       training_data: list[str],
       vocab_size: int = 32000,
       special_tokens: list[str] = ["<pad>", "<eos>", "<bos>"]
   ) -> tuple[list[bytes], dict[tuple[bytes, bytes], bytes]]:
       """
       è®­ç»ƒBPE tokenizer

       Args:
           training_data: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
           vocab_size: ç›®æ ‡è¯è¡¨å¤§å°ï¼ˆåŒ…æ‹¬special tokensï¼‰
           special_tokens: ç‰¹æ®Štokenåˆ—è¡¨

       Returns:
           vocab: è¯è¡¨ï¼ˆå­—èŠ‚åºåˆ—åˆ—è¡¨ï¼‰
           merges: åˆå¹¶è§„åˆ™å­—å…¸ {(pair): merged_token}
       """
   ```

   **å®ç°æ­¥éª¤**:
   1. å°†æ‰€æœ‰æ–‡æœ¬ç¼–ç ä¸ºå­—èŠ‚åºåˆ—
   2. ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡
   3. è¿­ä»£åˆå¹¶æœ€é«˜é¢‘å¯¹ï¼Œç›´åˆ°è¾¾åˆ°vocab_size
   4. è¿”å›è¯è¡¨å’Œåˆå¹¶è§„åˆ™

2. **`encode(text, vocab, merges)`** (0.5h)
   ```python
   def encode(
       text: str,
       vocab: list[bytes],
       merges: dict[tuple[bytes, bytes], bytes]
   ) -> list[int]:
       """
       ä½¿ç”¨BPEè§„åˆ™ç¼–ç æ–‡æœ¬

       Returns:
           token_ids: token IDåˆ—è¡¨
       """
   ```

**æµ‹è¯•**:
```bash
uv run pytest tests/test_train_bpe.py -v
uv run pytest tests/test_tokenizer.py -v
```

#### Part 2: Transformer Components (2å°æ—¶)

**æ–‡ä»¶**: `cs336_basics/model.py`

**éœ€è¦å®ç°çš„ç»„ä»¶**:

1. **Linearå±‚** (0.5h)
   ```python
   class Linear(nn.Module):
       def __init__(self, d_in: int, d_out: int):
           # ä½¿ç”¨æˆªæ–­æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
           std = sqrt(2 / (d_in + d_out))
           weight ~ trunc_normal(0, std, -3*std, 3*std)
   ```

2. **Embeddingå±‚** (0.5h)
   ```python
   class Embedding(nn.Module):
       def __init__(self, vocab_size: int, d_model: int):
           # åŒæ ·ä½¿ç”¨æˆªæ–­æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
   ```

3. **RMSNorm** (0.5h)
   ```python
   class RMSNorm(nn.Module):
       def forward(self, x):
           rms = sqrt(mean(xÂ², dim=-1, keepdim=True) + eps)
           return x / rms * self.weight
   ```

4. **Transformer Block** (0.5h)
   - Pre-normalizationæ¶æ„
   - Multi-Head Attention + FFN
   - æ®‹å·®è¿æ¥

**æµ‹è¯•**:
```bash
uv run pytest tests/test_model.py -v
```

#### Part 3: è®­ç»ƒä¸è¯„ä¼° (2å°æ—¶)

**æ–‡ä»¶**: `cs336_basics/optimizer.py`

**éœ€è¦å®ç°**:

1. **AdamWä¼˜åŒ–å™¨** (1h)
   ```python
   def adamw(
       params: list[nn.Parameter],
       grad: list[Tensor],
       lr: float = 1e-3,
       betas: tuple[float, float] = (0.9, 0.999),
       eps: float = 1e-8,
       weight_decay: float = 0.01
   ) -> None:
       """
       æ‰‹åŠ¨å®ç°AdamWæ›´æ–°è§„åˆ™
       """
   ```

2. **äº¤å‰ç†µæŸå¤±** (0.5h)
   ```python
   def cross_entropy(
       logits: Float[Tensor, "batch seq_len vocab_size"],
       targets: Int[Tensor, "batch seq_len"]
   ) -> Float[Tensor, ""]:
   ```

3. **è®­ç»ƒè„šæœ¬** (0.5h)
   - åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒ
   - è®¡ç®—perplexity
   - ä¿å­˜checkpoint

**æµ‹è¯•**:
```bash
uv run pytest tests/test_optimizer.py -v
```

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. BPEè®­ç»ƒä¼˜åŒ–

**é«˜æ•ˆç»Ÿè®¡å­—èŠ‚å¯¹**:
```python
from collections import Counter

def get_pair_frequencies(tokens_list):
    """ç»Ÿè®¡æ‰€æœ‰æ–‡æœ¬ä¸­çš„å­—èŠ‚å¯¹é¢‘ç‡"""
    pair_freqs = Counter()
    for tokens in tokens_list:
        for i in range(len(tokens) - 1):
            pair = (tokens[i], tokens[i+1])
            pair_freqs[pair] += 1
    return pair_freqs
```

### 2. RoPEå®ç°

```python
def apply_rotary_emb(x, cos, sin):
    """
    Args:
        x: [batch, seq_len, n_heads, head_dim]
        cos, sin: [seq_len, head_dim // 2]
    """
    # å°†xåˆ†æˆä¸¤åŠ
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]

    # åº”ç”¨æ—‹è½¬
    x_rotated = torch.cat([
        x1 * cos - x2 * sin,
        x1 * sin + x2 * cos
    ], dim=-1)

    return x_rotated
```

### 3. Flash AttentionåŸºç¡€

**é—®é¢˜**: æ ‡å‡†Attentionçš„å†…å­˜å¤æ‚åº¦O(NÂ²)

**è§£å†³æ–¹æ¡ˆ**: Tilingï¼ˆåˆ†å—è®¡ç®—ï¼‰
```python
# ä¼ªä»£ç 
def flash_attention(Q, K, V, block_size=64):
    # åˆ†å—è®¡ç®—ï¼Œå‡å°‘å†…å­˜å ç”¨
    for i in range(0, seq_len, block_size):
        for j in range(0, seq_len, block_size):
            Q_block = Q[:, i:i+block_size, :]
            K_block = K[:, j:j+block_size, :]
            V_block = V[:, j:j+block_size, :]

            # è®¡ç®—å±€éƒ¨attention
            S_block = Q_block @ K_block.T / sqrt(d)
            O_block = softmax(S_block) @ V_block

            # ç´¯ç§¯ç»“æœ
            O[:, i:i+block_size, :] += O_block
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### TinyStoriesæ•°æ®é›†

| æ¨¡å‹å¤§å° | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æœ€ç»ˆPerplexity | GPUè¦æ±‚ |
|---------|--------|---------|----------------|---------|
| Tiny    | 1M     | ~10min  | ~25            | 1 GPU   |
| Small   | 10M    | ~30min  | ~20            | 1 GPU   |
| Base    | 50M    | ~2h     | ~15            | 1-2 GPU |

### OpenWebTextå­é›†

| æ¨¡å‹å¤§å° | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æœ€ç»ˆPerplexity |
|---------|--------|---------|----------------|
| Tiny    | 1M     | ~20min  | ~35            |
| Small   | 10M    | ~1h     | ~28            |

---

## ğŸ¯ å­¦ä¹ æ£€éªŒ

### è‡ªæµ‹é¢˜

1. **BPE Tokenizer**:
   - Q: ä¸ºä»€ä¹ˆByte-Level BPEä¸éœ€è¦UNK tokenï¼Ÿ
   - Q: vocab_sizeä»32Ké™åˆ°16Kä¼šå½±å“ä»€ä¹ˆï¼Ÿ

2. **Transformeræ¶æ„**:
   - Q: RMSNormç›¸æ¯”LayerNormçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
   - Q: RoPEå¦‚ä½•æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Ÿ

3. **è®­ç»ƒæŠ€å·§**:
   - Q: æ¢¯åº¦ç´¯ç§¯å¦‚ä½•å®ç°ï¼Ÿ
   - Q: æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰çš„ä¼˜ç¼ºç‚¹ï¼Ÿ

### ä»£ç ç»ƒä¹ 

å®Œæˆ [examples.py](examples.py) ä¸­çš„ç»ƒä¹ é¢˜ã€‚

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**è®ºæ–‡**:
- "Attention Is All You Need" (Vaswani et al., 2017)
- "Language Models are Few-Shot Learners" (Brown et al., 2020)
- "Byte Pair Encoding is Suboptimal for Language Model Pretraining" (Bostrom et al., 2022)

**ä»£ç å‚è€ƒ**:
- [HuggingFace Transformers](https://github.com/huggingface/transformers)
- [nanoGPT](https://github.com/karpathy/nanoGPT)

---

## âš ï¸ å¸¸è§é™·é˜±

1. **BPEè®­ç»ƒ**:
   - âŒ å¿˜è®°å¤„ç†special tokens
   - âœ… ç¡®ä¿special tokensåœ¨è¯è¡¨å¼€å¤´

2. **RoPEå®ç°**:
   - âŒ ç»´åº¦é”™è¯¯ï¼ˆhead_diméœ€èƒ½è¢«2æ•´é™¤ï¼‰
   - âœ… ä½¿ç”¨`einops`è¿›è¡Œå¼ é‡æ“ä½œ

3. **è®­ç»ƒç¨³å®šæ€§**:
   - âŒ å­¦ä¹ ç‡è¿‡å¤§å¯¼è‡´NaN
   - âœ… ä½¿ç”¨warmup + weight decay

4. **å†…å­˜æ³„æ¼**:
   - âŒ æ²¡æœ‰é‡Šæ”¾ä¸­é—´å˜é‡
   - âœ… ä½¿ç”¨`del`å’Œ`torch.cuda.empty_cache()`

---

**ä¸‹ä¸€æ­¥**: [Day 17-18: Flash Attentionä¸DDP](../Day17-18_Flash_Attention_DDP/README.md)
