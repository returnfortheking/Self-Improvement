# Day 22-23: å‘é‡æ•°æ®åº“ä¸RAGåŸºç¡€

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡å‘é‡æ•°æ®åº“åŸç†ï¼Œå®ç°RAGåŸºç¡€æµç¨‹ï¼Œæ„å»ºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
> **æ—¶é—´åˆ†é…**: 6å°æ—¶ï¼ˆç†è®º2h + å®è·µ4hï¼‰
> **éš¾åº¦**: â­â­â­â­
> **é‡è¦æ€§**: â­â­â­â­â­ (RAGæ˜¯å½“å‰AIåº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯)

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. å‘é‡æ•°æ®åº“åŸºç¡€

#### 1.1 ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ

**ä¼ ç»Ÿæ•°æ®åº“ vs å‘é‡æ•°æ®åº“**:

| ç»´åº¦ | ä¼ ç»Ÿæ•°æ®åº“ (MySQL/MongoDB) | å‘é‡æ•°æ®åº“ (Chroma/Pinecone) |
|------|---------------------------|------------------------------|
| **æŸ¥è¯¢æ–¹å¼** | ç²¾ç¡®åŒ¹é… | ç›¸ä¼¼åº¦åŒ¹é… |
| **ç´¢å¼•** | B-Treeã€Hash | HNSWã€IVF |
| **æ•°æ®ç±»å‹** | ç»“æ„åŒ–æ•°æ® | å‘é‡ (Embedding) |
| **åº”ç”¨åœºæ™¯** | äº‹åŠ¡å¤„ç† | è¯­ä¹‰æœç´¢ã€æ¨è |

**æ ¸å¿ƒæ€æƒ³**:
```
æ–‡æœ¬ â†’ Embeddingæ¨¡å‹ â†’ å‘é‡ â†’ å‘é‡æ•°æ®åº“ â†’ ç›¸ä¼¼åº¦æœç´¢
```

#### 1.2 Embeddingï¼ˆåµŒå…¥ï¼‰

**å®šä¹‰**: å°†é«˜ç»´æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒï¼‰æ˜ å°„åˆ°ä½ç»´å‘é‡ç©ºé—´

**ç‰¹æ€§**:
- **è¯­ä¹‰ç›¸ä¼¼**: ç›¸ä¼¼æ–‡æœ¬çš„å‘é‡è·ç¦»è¿‘
- **å›ºå®šç»´åº¦**: å¦‚OpenAI text-embedding-3-small â†’ 1536ç»´
- **å¯†é›†å‘é‡**: æ¯ä¸ªç»´åº¦éƒ½æœ‰æ„ä¹‰

**ç¤ºä¾‹**:
```python
# æ–‡æœ¬
text1 = "æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯"
text2 = "æ·±åº¦å­¦ä¹ å±äºæœºå™¨å­¦ä¹ "
text3 = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"

# Embeddingå
vec1 = [0.12, -0.34, 0.56, ...]  # 1536ç»´
vec2 = [0.13, -0.32, 0.54, ...]  # ä¸vec1ç›¸ä¼¼ï¼ˆè·ç¦»è¿‘ï¼‰
vec3 = [-0.45, 0.78, -0.23, ...]  # ä¸vec1ä¸ç›¸ä¼¼ï¼ˆè·ç¦»è¿œï¼‰
```

#### 1.3 ç›¸ä¼¼åº¦åº¦é‡

**å¸¸ç”¨è·ç¦»å…¬å¼**:

1. **ä½™å¼¦ç›¸ä¼¼åº¦** (æœ€å¸¸ç”¨):
   ```
   cos_sim(A, B) = (A Â· B) / (||A|| Ã— ||B||)
   èŒƒå›´: [-1, 1]ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼
   ```

2. **æ¬§æ°è·ç¦»**:
   ```
   euclidean(A, B) = sqrt(Î£(Ai - Bi)Â²)
   èŒƒå›´: [0, +âˆ)ï¼Œè¶Šå°è¶Šç›¸ä¼¼
   ```

3. **ç‚¹ç§¯**:
   ```
   dot(A, B) = Î£(Ai Ã— Bi)
   èŒƒå›´: [-âˆ, +âˆ]ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼
   ```

**é€‰æ‹©å»ºè®®**:
- æ–‡æœ¬æœç´¢: **ä½™å¼¦ç›¸ä¼¼åº¦**ï¼ˆå½’ä¸€åŒ–ï¼Œä¸å—é•¿åº¦å½±å“ï¼‰
- å›¾åƒæ£€ç´¢: æ¬§æ°è·ç¦»
- æ¨èç³»ç»Ÿ: ç‚¹ç§¯ï¼ˆå¿«é€Ÿè®¡ç®—ï¼‰

---

### 2. RAG (Retrieval-Augmented Generation) åŸç†

#### 2.1 ä¸ºä»€ä¹ˆéœ€è¦RAGï¼Ÿ

**LLMçš„é—®é¢˜**:
1. **çŸ¥è¯†æˆªæ­¢**: è®­ç»ƒæ•°æ®æœ‰æˆªæ­¢æ—¥æœŸ
2. **å¹»è§‰**: å¯èƒ½ç¼–é€ é”™è¯¯ä¿¡æ¯
3. **ç§æœ‰æ•°æ®**: æ— æ³•è®¿é—®ä¼ä¸šå†…éƒ¨æ–‡æ¡£

**RAGè§£å†³æ–¹æ¡ˆ**:
```
ç”¨æˆ·æŸ¥è¯¢ â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ LLMåŸºäºæ–‡æ¡£ç”Ÿæˆå›ç­”
```

#### 2.2 RAGæ¶æ„

**å®Œæ•´æµç¨‹**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG Pipeline                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. æ–‡æ¡£åŠ è½½ (Document Loading)                          â”‚
â”‚     â”œâ”€ PDF, Markdown, TXT                               â”‚
â”‚     â””â”€ ç½‘é¡µæŠ“å–                                         â”‚
â”‚                                                          â”‚
â”‚  2. æ–‡æ¡£åˆ†å— (Chunking)                                  â”‚
â”‚     â”œâ”€ å›ºå®šå¤§å°åˆ†å— (512 tokens)                         â”‚
â”‚     â”œâ”€ è¯­ä¹‰åˆ†å— (æŒ‰æ®µè½ã€ç« èŠ‚)                           â”‚
â”‚     â””â”€ é‡å åˆ†å— (overlap=50)                             â”‚
â”‚                                                          â”‚
â”‚  3. å‘é‡åŒ– (Embedding)                                   â”‚
â”‚     â””â”€ OpenAI, BGE, MTEBæ¨¡å‹                            â”‚
â”‚                                                          â”‚
â”‚  4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ (Vector Store)                       â”‚
â”‚     â””â”€ Chroma, Pinecone, Weaviate                        â”‚
â”‚                                                          â”‚
â”‚  5. æ£€ç´¢ (Retrieval)                                     â”‚
â”‚     â”œâ”€ å‘é‡æ£€ç´¢ (Vector Search)                          â”‚
â”‚     â”œâ”€ æ··åˆæ£€ç´¢ (Hybrid: Vector + BM25)                 â”‚
â”‚     â””â”€ é‡æ’åº (Rerank)                                   â”‚
â”‚                                                          â”‚
â”‚  6. ç”Ÿæˆ (Generation)                                    â”‚
â”‚     â””â”€ LLMåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3. Chromaå‘é‡æ•°æ®åº“

#### 3.1 ChromaåŸºç¡€

**ç‰¹ç‚¹**:
- âœ… å¼€æºã€è½»é‡çº§
- âœ… æœ¬åœ°éƒ¨ç½²ï¼ˆæ— éœ€API keyï¼‰
- âœ… PythonåŸç”Ÿæ”¯æŒ
- âœ… æŒä¹…åŒ–å­˜å‚¨

**å®‰è£…**:
```bash
pip install chromadb
```

**åŸºç¡€æ“ä½œ**:
```python
import chromadb

# 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
client = chromadb.PersistentClient(path="./data/chroma")

# 2. åˆ›å»º/è·å–é›†åˆ
collection = client.get_or_create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
)

# 3. æ·»åŠ æ–‡æ¡£
collection.add(
    documents=["è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬", "è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬"],
    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...]],  # å¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆ
    metadatas=[{"source": "doc1"}, {"source": "doc2"}],
    ids=["doc1", "doc2"]
)

# 4. æŸ¥è¯¢
results = collection.query(
    query_texts=["ç”¨æˆ·æŸ¥è¯¢"],
    n_results=5  # è¿”å›top 5
)

# 5. åˆ é™¤
collection.delete(ids=["doc1"])
```

#### 3.2 é«˜çº§åŠŸèƒ½

**å…ƒæ•°æ®è¿‡æ»¤**:
```python
# åªæ£€ç´¢ç‰¹å®šæ¥æºçš„æ–‡æ¡£
results = collection.query(
    query_texts=["æŸ¥è¯¢"],
    where={"source": "doc1"},  # ç²¾ç¡®åŒ¹é…
    n_results=5
)

# å¤æ‚æ¡ä»¶
results = collection.query(
    query_texts=["æŸ¥è¯¢"],
    where={
        "$and": [
            {"source": {"$ne": "tmp"}},  # source != "tmp"
            {"date": {"$gte": "2024-01-01"}}
        ]
    }
)
```

**è‡ªåŠ¨Embedding**:
```python
import chromadb
from chromadb.utils import embedding_functions

# ä½¿ç”¨OpenAI Embedding
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="your-key",
    model_name="text-embedding-3-small"
)

client = chromadb.Client()
collection = client.get_or_create_collection(
    name="docs",
    embedding_function=openai_ef
)

# è‡ªåŠ¨embed
collection.add(
    documents=["æ–‡æœ¬1", "æ–‡æœ¬2"],
    ids=["doc1", "doc2"]  # æ— éœ€æ‰‹åŠ¨æä¾›embeddings
)
```

---

### 4. æ–‡æ¡£å¤„ç†

#### 4.1 æ–‡æ¡£åŠ è½½

**ä½¿ç”¨LangChain**:
```python
from langchain.document_loaders import PyPDFLoader, DirectoryLoader

# åŠ è½½å•ä¸ªPDF
loader = PyPDFLoader("data/report.pdf")
pages = loader.load()

# åŠ è½½æ•´ä¸ªç›®å½•
loader = DirectoryLoader(
    "data/documents",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader
)
docs = loader.load()
```

**ä½¿ç”¨unstructured** (æ›´å¼ºå¤§):
```python
from unstructured.partition.pdf import partition_pdf

# æ”¯æŒå¤æ‚å¸ƒå±€ï¼ˆè¡¨æ ¼ã€å›¾ç‰‡ï¼‰
elements = partition_pdf(
    filename="report.pdf",
    extract_images_in_pdf=True,
    infer_table_structure=True
)
```

#### 4.2 æ–‡æ¡£åˆ†å—ç­–ç•¥

**ç­–ç•¥1: å›ºå®šé•¿åº¦åˆ†å—**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,        # æ¯å—512å­—ç¬¦
    chunk_overlap=50,      # é‡å 50å­—ç¬¦ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
    separators=["\n\n", "\n", "ã€‚", " ", ""]
)

chunks = splitter.split_documents(docs)
```

**ç­–ç•¥2: è¯­ä¹‰åˆ†å—**
```python
from langchain_experimental.text_splitter import SemanticChunker

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—
splitter = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile"
)

chunks = splitter.split_text(text)
```

**ç­–ç•¥3: è‡ªå®šä¹‰åˆ†å—**
```python
def custom_chunker(text, max_length=512, overlap=50):
    """è‡ªå®šä¹‰åˆ†å—é€»è¾‘"""
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_length
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # é‡å 
    return chunks
```

**åˆ†å—æœ€ä½³å®è·µ**:
| åœºæ™¯ | chunk_size | overlap | åˆ†éš”ç¬¦ |
|------|-----------|---------|--------|
| é•¿æ–‡æ¡£ | 1024-2048 | 128-256 | æ®µè½ |
| ä»£ç  | 512 | 50 | å‡½æ•°/ç±» |
| QAå¯¹ | 256-512 | 0 | é—®é¢˜ |

---

## ğŸ”§ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: æ„å»ºPDFæ–‡æ¡£RAGç³»ç»Ÿ

**å®Œæ•´æµç¨‹**:

```python
import chromadb
from chromadb.utils import embedding_functions
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai

# 1. åˆå§‹åŒ–ç»„ä»¶
class PDFRAGSystem:
    def __init__(self):
        # ChromaDB
        self.client = chromadb.PersistentClient(path="./data/db")
        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
            api_key="your-key",
            model_name="text-embedding-3-small"
        )
        self.collection = self.client.get_or_create_collection(
            name="pdf_docs",
            embedding_function=self.embedding_fn
        )

        # æ–‡æ¡£åˆ†å—
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=50
        )

    def ingest_pdf(self, pdf_path):
        """åŠ è½½PDFå¹¶ç´¢å¼•"""
        # åŠ è½½PDF
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()

        # åˆ†å—
        chunks = []
        for page in pages:
            page_chunks = self.splitter.split_text(page.page_content)
            for i, chunk in enumerate(page_chunks):
                chunks.append({
                    "text": chunk,
                    "metadata": {
                        "source": pdf_path,
                        "page": page.metadata["page"],
                        "chunk": i
                    }
                })

        # å­˜å‚¨åˆ°Chroma
        self.collection.add(
            documents=[c["text"] for c in chunks],
            metadatas=[c["metadata"] for c in chunks],
            ids=[f"{pdf_path}_{i}" for i in range(len(chunks))]
        )

        print(f"âœ… ç´¢å¼•å®Œæˆ: {len(chunks)}ä¸ªchunks")

    def query(self, question, top_k=5):
        """æŸ¥è¯¢å¹¶å›ç­”"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        results = self.collection.query(
            query_texts=[question],
            n_results=top_k
        )

        # 2. æ„å»ºprompt
        context = "\n\n".join(results["documents"][0])
        prompt = f"""
        åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

        æ–‡æ¡£:
        {context}

        é—®é¢˜: {question}
        ç­”æ¡ˆ:
        """

        # 3. LLMç”Ÿæˆ
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )

        return response.choices[0].message.content

# ä½¿ç”¨
rag = PDFRAGSystem()
rag.ingest_pdf("data/report.pdf")
answer = rag.query("æŠ¥å‘Šçš„ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ")
print(answer)
```

---

### æ¡ˆä¾‹2: æ··åˆæ£€ç´¢ï¼ˆVector + BM25ï¼‰

```python
from rank_bm25 import BM25Okapi
import jieba

class HybridRAG:
    def __init__(self):
        # å‘é‡æ£€ç´¢
        self.client = chromadb.PersistentClient(path="./data/db")
        self.collection = self.client.get_or_create_collection("docs")

        # BM25æ£€ç´¢
        self.bm25_index = None
        self.docs = []

    def index(self, documents):
        """æ„å»ºæ··åˆç´¢å¼•"""
        self.docs = documents

        # 1. å‘é‡ç´¢å¼•
        self.collection.add(
            documents=documents,
            ids=[f"doc_{i}" for i in range(len(documents))]
        )

        # 2. BM25ç´¢å¼•
        tokenized_docs = [list(jieba.cut(doc)) for doc in documents]
        self.bm25_index = BM25Okapi(tokenized_docs)

    def search(self, query, top_k=5, alpha=0.5):
        """
        æ··åˆæ£€ç´¢

        Args:
            alpha: å‘é‡æ£€ç´¢æƒé‡ (0-1)
                   alpha=1: çº¯å‘é‡æ£€ç´¢
                   alpha=0: çº¯BM25æ£€ç´¢
                   alpha=0.5: åŠ æƒèåˆ
        """
        # 1. å‘é‡æ£€ç´¢
        vector_results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )
        vector_scores = {id_: 1-i/top_k for i, id_ in enumerate(vector_results["ids"][0])}

        # 2. BM25æ£€ç´¢
        tokenized_query = list(jieba.cut(query))
        bm25_results = self.bm25_index.get_top_n(tokenized_query, self.docs, n=top_k)
        bm25_scores = {doc: score for doc, score in bm25_results}

        # 3. èåˆæ‰“åˆ†
        final_scores = {}
        for doc_id in vector_scores:
            final_scores[doc_id] = (
                alpha * vector_scores[doc_id] +
                (1-alpha) * bm25_scores.get(doc_id, 0)
            )

        # 4. æ’åº
        ranked_docs = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)

        return ranked_docs[:top_k]
```

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. Embeddingä¼˜åŒ–

**æ‰¹é‡å¤„ç†**:
```python
def batch_embed(texts, batch_size=100):
    """æ‰¹é‡Embeddingï¼ˆåŠ é€Ÿï¼‰"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_embeddings = embedding_model.embed_documents(batch)
        embeddings.extend(batch_embeddings)
    return embeddings
```

**ç¼“å­˜Embedding**:
```python
import hashlib
import pickle

class CachedEmbedding:
    def __init__(self, model):
        self.model = model
        self.cache = {}

    def embed(self, text):
        # ç”Ÿæˆhash
        text_hash = hashlib.md5(text.encode()).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        if text_hash in self.cache:
            return self.cache[text_hash]

        # è®¡ç®—å¹¶ç¼“å­˜
        embedding = self.model.embed_query(text)
        self.cache[text_hash] = embedding
        return embedding
```

### 2. åˆ†å—ä¼˜åŒ–

**æ»‘åŠ¨çª—å£åˆ†å—**:
```python
def sliding_window_chunks(text, window_size=512, stride=256):
    """æ»‘åŠ¨çª—å£åˆ†å—ï¼ˆæ›´å¤šä¿¡æ¯ä¿ç•™ï¼‰"""
    chunks = []
    for i in range(0, len(text), stride):
        chunk = text[i:i+window_size]
        if len(chunk) > 0:
            chunks.append(chunk)
    return chunks
```

**è¯­ä¹‰è¾¹ç•Œæ£€æµ‹**:
```python
def semantic_aware_chunk(text, max_length=512):
    """åœ¨è¯­ä¹‰è¾¹ç•Œå¤„åˆ†å—"""
    sentences = text.split("ã€‚")
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) > max_length:
            chunks.append(current_chunk)
            current_chunk = sentence
        else:
            current_chunk += sentence + "ã€‚"

    if current_chunk:
        chunks.append(current_chunk)

    return chunks
```

### 3. æ£€ç´¢ä¼˜åŒ–

**Rerankï¼ˆé‡æ’åºï¼‰**:
```python
from sentence_transformers import CrossEncoder

def retrieve_with_rerank(query, top_k=100, rerank_top=10):
    """ä¸¤é˜¶æ®µæ£€ç´¢"""
    # 1. ç²—æ’ï¼ˆå¬å›ï¼‰
    candidates = vector_store.search(query, k=top_k)

    # 2. ç²¾æ’ï¼ˆRerankï¼‰
    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

    pairs = [[query, doc["text"]] for doc in candidates]
    scores = reranker.predict(pairs)

    # 3. é‡æ–°æ’åº
    reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)

    return [doc for doc, score in reranked[:rerank_top]]
```

**æŸ¥è¯¢æ‰©å±•**:
```python
def query_expansion(query):
    """æŸ¥è¯¢æ‰©å±•ï¼ˆæé«˜å¬å›ç‡ï¼‰"""
    # åŒä¹‰è¯æ‰©å±•
    expansions = []
    if "æœºå™¨å­¦ä¹ " in query:
        expansions.append("AI")
        expansions.append("æ·±åº¦å­¦ä¹ ")

    # é‡å†™
    expanded_query = f"{query} {' '.join(expansions)}"

    return expanded_query
```

---

## ğŸ“Š æ€§èƒ½è¯„ä¼°

### RAGè¯„ä¼°æŒ‡æ ‡

**æ£€ç´¢è´¨é‡**:
1. **å¬å›ç‡ (Recall)**: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ / æ‰€æœ‰ç›¸å…³æ–‡æ¡£
2. **ç²¾ç¡®ç‡ (Precision)**: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ / æ£€ç´¢åˆ°çš„æ€»æ–‡æ¡£
3. **MRR (Mean Reciprocal Rank)**: ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„å€’æ•°æ’å

**ç”Ÿæˆè´¨é‡**:
1. **å¿ å®åº¦ (Faithfulness)**: ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£
2. **ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevance)**: ç­”æ¡ˆæ˜¯å¦å›ç­”äº†é—®é¢˜

**è¯„ä¼°å·¥å…·**:
```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

results = evaluate(
    dataset=test_dataset,
    metrics=[faithfulness, answer_relevancy]
)

print(results)
```

---

## ğŸ¯ å­¦ä¹ æ£€éªŒ

### å…³é”®é—®é¢˜

1. **å‘é‡æ•°æ®åº“**:
   - å‘é‡æ•°æ®åº“ä¸ä¼ ç»Ÿæ•°æ®åº“çš„åŒºåˆ«ï¼Ÿ
   - å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç›¸ä¼¼åº¦åº¦é‡ï¼Ÿ
   - Chromaçš„é«˜çº§åŠŸèƒ½æœ‰å“ªäº›ï¼Ÿ

2. **RAGæ¶æ„**:
   - RAGçš„å®Œæ•´æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ
   - å¦‚ä½•ä¼˜åŒ–æ–‡æ¡£åˆ†å—ç­–ç•¥ï¼Ÿ
   - æ··åˆæ£€ç´¢çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

3. **å®ç°ç»†èŠ‚**:
   - å¦‚ä½•æ„å»ºç”Ÿäº§çº§RAGç³»ç»Ÿï¼Ÿ
   - å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿæ€§èƒ½ï¼Ÿ
   - å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼Ÿ

### ä»£ç ç»ƒä¹ 

å®Œæˆ [examples.py](examples.py) ä¸­çš„ç»ƒä¹ é¢˜ã€‚

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**æ–‡æ¡£**:
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)
- [RAGAS Evaluation](https://docs.ragas.io/)

**è®ºæ–‡**:
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al., 2020)
- "Indexify: A Framework for Building RAG Applications" (2023)

**ä»£ç **:
- [MODULAR-RAG-MCP-SERVER](references/github/MODULAR-RAG-MCP-SERVER/) - å®Œæ•´RAGå®ç°

---

## âš ï¸ å¸¸è§é™·é˜±

1. **åˆ†å—ç­–ç•¥ä¸å½“**:
   - âŒ chunk_sizeå¤ªå°ï¼ˆä¸¢å¤±ä¸Šä¸‹æ–‡ï¼‰
   - âŒ chunk_sizeå¤ªå¤§ï¼ˆæ£€ç´¢ä¸ç²¾ç¡®ï¼‰
   - âœ… æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´ï¼ˆ512-2048ï¼‰

2. **Embeddingæ¨¡å‹é€‰æ‹©**:
   - âŒ ä½¿ç”¨é€šç”¨æ¨¡å‹å¤„ç†ä¸“ä¸šæ–‡æ¡£
   - âœ… ä½¿ç”¨é¢†åŸŸå¾®è°ƒçš„æ¨¡å‹

3. **æ£€ç´¢æ•°é‡**:
   - âŒ top_kå¤ªå¤§ï¼ˆå¼•å…¥å™ªå£°ï¼‰
   - âŒ top_kå¤ªå°ï¼ˆé—æ¼å…³é”®ä¿¡æ¯ï¼‰
   - âœ… æ ¹æ®ä»»åŠ¡è°ƒæ•´ï¼ˆ3-10ï¼‰

4. **Promptå·¥ç¨‹**:
   - âŒ åªæä¾›é—®é¢˜ï¼Œä¸æä¾›ä¸Šä¸‹æ–‡
   - âœ… æ˜ç¡®æŒ‡ç¤º"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”"

---

**ä¸‹ä¸€æ­¥**: [Day 24-25: Scaling Laws](../Day24-25_Scaling_Laws/README.md)
