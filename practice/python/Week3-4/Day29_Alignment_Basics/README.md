# Day 29: Assignment 5 - Alignment & RLHF

> **å­¦ä¹ ç›®æ ‡**: ç†è§£å¯¹é½(Alignment)é—®é¢˜ï¼ŒæŒæ¡RLHFå’ŒDPOæ–¹æ³•ï¼Œå®ç°äººç±»åé¦ˆè®­ç»ƒ
> **æ—¶é—´åˆ†é…**: 6å°æ—¶ï¼ˆç†è®º2.5h + å®è·µ3.5hï¼‰
> **éš¾åº¦**: â­â­â­â­â­
> **é‡è¦æ€§**: â­â­â­â­â­ (LLMå®‰å…¨æ€§å’Œå¯æ§æ€§çš„æ ¸å¿ƒæŠ€æœ¯)
> **æ¥æº**: CS336 Assignment 5 - Alignment

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. å¯¹é½é—®é¢˜(Alignment Problem)

#### 1.1 ä»€ä¹ˆæ˜¯å¯¹é½ï¼Ÿ

**å®šä¹‰**: ç¡®ä¿AIç³»ç»Ÿçš„è¡Œä¸ºä¸äººç±»ä»·å€¼è§‚ã€æ„å›¾å’ŒæœŸæœ›ä¿æŒä¸€è‡´

**ä¸‰ä¸ªå±‚é¢**:
1. **æ„å›¾å¯¹é½**: AIç†è§£å¹¶æ‰§è¡Œç”¨æˆ·æƒ³è¦çš„æ“ä½œ
2. **ä»·å€¼è§‚å¯¹é½**: AIçš„è¡Œä¸ºç¬¦åˆç¤¾ä¼šé“å¾·æ ‡å‡†
3. **å®‰å…¨æ€§å¯¹é½**: AIä¸ä¼šäº§ç”Ÿæœ‰å®³å†…å®¹

**ä¸ºä»€ä¹ˆé‡è¦**:
```
æœªå¯¹é½çš„é£é™©:
- ç”Ÿæˆæœ‰å®³å†…å®¹ï¼ˆæš´åŠ›ã€æ­§è§†ã€è™šå‡ä¿¡æ¯ï¼‰
- ç›®æ ‡é”™è¯¯ä¼˜åŒ–ï¼ˆ"å›å½¢é’ˆæœ€å¤§åŒ–å™¨"æ€æƒ³å®éªŒï¼‰
- å¯¹æŠ—æ€§æ”»å‡»ï¼ˆæç¤ºè¯æ³¨å…¥ï¼‰
- ä¸å¯æ§çš„è¡Œä¸ºï¼ˆæ¬ºéª—æ€§ã€é€ƒé¿ç›‘ç®¡ï¼‰

å¯¹é½çš„ä»·å€¼:
- æå‡ç”¨æˆ·ä½“éªŒå’Œä¿¡ä»»
- æ»¡è¶³ç›‘ç®¡è¦æ±‚ï¼ˆAIæ³•æ¡ˆï¼‰
- å•†ä¸šåº”ç”¨çš„å¿…è¦æ¡ä»¶
- AGIå‘å±•çš„å®‰å…¨ä¿éšœ
```

#### 1.2 å¯¹é½æ–¹æ³•æ¼”è¿›

**å†å²è„‰ç»œ**:
```
2017: RLHFé›å½¢ï¼ˆOpenAIï¼‰
   â†“
2020: GPT-3åº”ç”¨InstructGPTæŠ€æœ¯
   â†“
2022: ChatGPTæˆåŠŸï¼ˆRLHFè§„æ¨¡åŒ–ï¼‰
   â†“
2023: DPOç­‰æ›¿ä»£æ–¹æ³•å‡ºç°
   â†“
2024: Constitutional AI, ORPOç­‰æ–°æ–¹å‘
```

**ä¸»æµæ–¹æ³•å¯¹æ¯”**:

| æ–¹æ³• | åŸç† | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| **Supervised Fine-tuning** | äººç±»æ ‡æ³¨æ•°æ®ç›´æ¥è®­ç»ƒ | ç®€å•é«˜æ•ˆ | ä¾èµ–æ ‡æ³¨è´¨é‡ | åŸºç¡€å¯¹é½ |
| **RLHF** | åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  | æ•ˆæœæ˜¾è‘— | è®­ç»ƒå¤æ‚ | ç”Ÿäº§ç¯å¢ƒ |
| **DPO** | ç›´æ¥åå¥½ä¼˜åŒ– | æ— éœ€å¥–åŠ±æ¨¡å‹ | éœ€è¦æˆå¯¹æ•°æ® | ç ”ç©¶å®éªŒ |
| **RAG** | æ£€ç´¢å¢å¼ºç”Ÿæˆ | äº‹å®å‡†ç¡® | éœ€è¦å¤–éƒ¨çŸ¥è¯†åº“ | çŸ¥è¯†å¯†é›†å‹ |
| **Constitutional AI** | åŸºäºåŸåˆ™çš„è‡ªæˆ‘ä¿®æ­£ | å¯è§£é‡Šæ€§å¼º | è®¡ç®—å¼€é”€å¤§ | å®‰å…¨å…³é”® |

---

### 2. RLHF (Reinforcement Learning from Human Feedback)

#### 2.1 RLHFä¸‰é˜¶æ®µæµç¨‹

**é˜¶æ®µ1: ç›‘ç£å¾®è°ƒ(SFT)**
```
ç›®æ ‡: è®­ç»ƒåŸºç¡€æ¨¡å‹ç†è§£æŒ‡ä»¤

æ•°æ®:
- Prompt: "è§£é‡Šé‡å­è®¡ç®—"
- Response: "é‡å­è®¡ç®—åˆ©ç”¨é‡å­æ¯”ç‰¹..."

è®­ç»ƒ:
- ä½¿ç”¨é«˜è´¨é‡æŒ‡ä»¤æ•°æ®
- é€šå¸¸10K-100Kæ ·æœ¬
- æ ‡å‡†çš„è¯­è¨€æ¨¡å‹æŸå¤±å‡½æ•°
```

**é˜¶æ®µ2: å¥–åŠ±æ¨¡å‹(Reward Model)è®­ç»ƒ**
```
ç›®æ ‡: å­¦ä¹ äººç±»åå¥½

æ•°æ®æ”¶é›†:
Prompt              |  Response A       |  Response B       | Preference
"è§£é‡Šé‡å­è®¡ç®—"      | "é‡å­è®¡ç®—æ˜¯..."   | "é‡å­æ¯”ç‰¹æ˜¯..."   | A > B
"å†™ä¸€é¦–è¯—"         | "æ˜¥å¤©æ¥äº†..."     | "è¯—æ­Œæ˜¯..."       | B > A

è®­ç»ƒ:
- è¾“å…¥: (prompt, response)å¯¹
- è¾“å‡º: æ ‡é‡å¥–åŠ±å€¼ï¼ˆæ‰“åˆ†ï¼‰
- æŸå¤±: æˆå¯¹æ’åºæŸå¤±
```

**é˜¶æ®µ3: PPOå¼ºåŒ–å­¦ä¹ **
```
ç›®æ ‡: ä¼˜åŒ–ç­–ç•¥ä»¥æœ€å¤§åŒ–å¥–åŠ±

æµç¨‹:
1. ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆresponse
2. å¥–åŠ±æ¨¡å‹å¯¹responseæ‰“åˆ†
3. è®¡ç®—PPOæŸå¤±
4. æ›´æ–°ç­–ç•¥å‚æ•°
5. é‡å¤1-4ï¼ˆå¤šä¸ªepochï¼‰

çº¦æŸ:
- KLæ•£åº¦æƒ©ç½šï¼ˆé˜²æ­¢åç¦»SFTæ¨¡å‹å¤ªè¿œï¼‰
- ä»·å€¼å‡½æ•°è£å‰ª
- ä¿¡ä»»åŒºåŸŸä¼˜åŒ–
```

#### 2.2 å¥–åŠ±æ¨¡å‹(Reward Model)

**æ¨¡å‹ç»“æ„**:
```python
class RewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base = base_model  # å…±äº«åŸºç¡€æ¨¡å‹
        self.reward_head = nn.Linear(hidden_size, 1)  # å¥–åŠ±å¤´

    def forward(self, input_ids, attention_mask):
        outputs = self.base(input_ids, attention_mask)
        last_hidden = outputs.last_hidden_state[:, -1, :]
        reward = self.reward_head(last_hidden)
        return reward
```

**è®­ç»ƒæŸå¤±ï¼ˆBradley-Terryæ¨¡å‹ï¼‰**:
```python
def reward_loss(reward_chosen, reward_rejected):
    """
    è®¡ç®—æˆå¯¹æ’åºæŸå¤±

    ç›®æ ‡: ä½¿chosençš„å¥–åŠ± > rejectedçš„å¥–åŠ±
    """
    # Logæ¦‚ç‡
    prob_chosen = torch.logsigmoid(reward_chosen - reward_rejected)

    # æŸå¤±ï¼ˆæœ€å¤§åŒ–chosençš„å¥–åŠ±ï¼‰
    loss = -prob_chosen.mean()

    # å‡†ç¡®ç‡
    accuracy = (reward_chosen > reward_rejected).float().mean()

    return loss, accuracy
```

**å…³é”®è¶…å‚æ•°**:
- **å­¦ä¹ ç‡**: 1e-5 ~ 5e-5ï¼ˆè¾ƒå°ï¼Œé¿å…ç ´åé¢„è®­ç»ƒæƒé‡ï¼‰
- **Batch size**: 64 ~ 256
- **Epoch**: 1 ~ 3ï¼ˆè¿‡æ‹Ÿåˆé£é™©ï¼‰
- **æ¸©åº¦å‚æ•°**: 0.1 ~ 1.0ï¼ˆæ§åˆ¶å¥–åŠ±åˆ†å¸ƒï¼‰

#### 2.3 PPOç®—æ³• (Proximal Policy Optimization)

**æ ¸å¿ƒæ€æƒ³**: åœ¨ä¿¡ä»»åŒºåŸŸå†…ä¼˜åŒ–ç­–ç•¥ï¼Œé¿å…ç­–ç•¥æ›´æ–°è¿‡å¤§

**ç›®æ ‡å‡½æ•°**:
```
L(Î¸) = E[ min( r(Î¸) * A, clip(r(Î¸), 1-Îµ, 1+Îµ) * A ) ]

å…¶ä¸­:
- r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s) ï¼ˆæ¦‚ç‡æ¯”ç‡ï¼‰
- A: ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰
- Îµ: è£å‰ªå‚æ•°ï¼ˆé€šå¸¸0.2ï¼‰
```

**ä¼˜åŠ¿å‡½æ•°è®¡ç®—**:
```python
def compute_advantages(rewards, values, gamma=0.99, lambda_gae=0.95):
    """
    ä½¿ç”¨GAE (Generalized Advantage Estimation)è®¡ç®—ä¼˜åŠ¿
    """
    advantages = []
    gae = 0

    # ä»åå‘å‰è®¡ç®—
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]

        # TDæ®‹å·®
        delta = rewards[t] + gamma * next_value - values[t]

        # GAE
        gae = delta + gamma * lambda_gae * gae
        advantages.insert(0, gae)

    return torch.tensor(advantages)
```

**PPOæŸå¤±å‡½æ•°**:
```python
def ppo_loss(policy_log_probs, old_policy_log_probs, advantages,
             value_pred, returns, clip_param=0.2):
    """
    è®¡ç®—PPOæŸå¤±
    """
    # 1. ç­–ç•¥æŸå¤±ï¼ˆè£å‰ªï¼‰
    ratio = torch.exp(policy_log_probs - old_policy_log_probs)
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()

    # 2. ä»·å€¼å‡½æ•°æŸå¤±
    value_loss = F.mse_loss(value_pred, returns)

    # 3. ç†µå¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
    entropy_bonus = -policy_log_probs.mean()

    # æ€»æŸå¤±
    total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus

    return total_loss
```

**KLæ•£åº¦æƒ©ç½š**:
```python
def kl_penalty(log_probs, ref_log_probs):
    """
    è®¡ç®—KLæ•£åº¦æƒ©ç½šï¼Œé˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒç­–ç•¥å¤ªè¿œ
    """
    kl_div = log_probs - ref_log_probs
    return kl_div.mean()

# åœ¨æ€»æŸå¤±ä¸­æ·»åŠ 
total_loss = ppo_loss + kl_coeff * kl_penalty
```

---

### 3. DPO (Direct Preference Optimization)

#### 3.1 DPOåŸç†

**æ ¸å¿ƒæ€æƒ³**: ç›´æ¥ä¼˜åŒ–åå¥½æ•°æ®ï¼Œæ— éœ€æ˜¾å¼çš„å¥–åŠ±æ¨¡å‹

**æ¨å¯¼**:
```
ä¼ ç»ŸRLHF:
1. è®­ç»ƒå¥–åŠ±æ¨¡å‹ R(x,y)
2. ç”¨RLä¼˜åŒ– max E[R(x,y)]

DPO:
ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å¾—:
Ï€(y_chosen|x) / Ï€(y_rejected|x) âˆ exp(R(x,y_chosen) - R(x,y_rejected))

å³: æœ€å¤§åŒ– log(Ï€(y_chosen|x)) - log(Ï€(y_rejected|x))
```

**DPOæŸå¤±å‡½æ•°**:
```python
def dpo_loss(policy_chosen_logps, policy_rejected_logps,
             ref_chosen_logps, ref_rejected_logps, beta=0.1):
    """
    DPOæŸå¤±å‡½æ•°

    ç›®æ ‡: æé«˜chosençš„log_probï¼Œé™ä½rejectedçš„log_prob
    """
    # ç­–ç•¥æ¨¡å‹çš„logæ¦‚ç‡å·®
    policy_logratios = policy_chosen_logps - policy_rejected_logps

    # å‚è€ƒæ¨¡å‹çš„logæ¦‚ç‡å·®
    ref_logratios = ref_chosen_logps - ref_rejected_logps

    # DPOæŸå¤±
    losses = -F.logsigmoid(beta * (policy_logratios - ref_logratios))

    # æ ‡ç­¾ï¼ˆchosen > rejectedï¼‰
    labels = torch.zeros(losses.size())

    # äº¤å‰ç†µæŸå¤±
    loss = F.binary_cross_entropy_with_logits(
        -beta * (policy_logratios - ref_logratios),
        labels
    )

    # å‡†ç¡®ç‡
    acc = (policy_chosen_logps > policy_rejected_logps).float().mean()

    return loss, acc
```

**DPO vs RLHFå¯¹æ¯”**:

| ç»´åº¦ | RLHF | DPO |
|------|------|-----|
| **å¥–åŠ±æ¨¡å‹** | éœ€è¦å•ç‹¬è®­ç»ƒ | æ— éœ€ |
| **å¼ºåŒ–å­¦ä¹ ** | éœ€è¦PPOç­‰RLç®—æ³• | ç›´æ¥ä¼˜åŒ– |
| **è®­ç»ƒç¨³å®šæ€§** | è¾ƒéš¾è°ƒå‚ | æ›´ç¨³å®š |
| **è®¡ç®—æ•ˆç‡** | è¾ƒä½ï¼ˆå¤šé˜¶æ®µï¼‰ | è¾ƒé«˜ |
| **æ•ˆæœ** | SOTA | æ¥è¿‘RLHF |
| **é€‚ç”¨åœºæ™¯** | ç”Ÿäº§ç¯å¢ƒ | ç ”ç©¶/å¿«é€Ÿè¿­ä»£ |

#### 3.2 DPOå®ç°

**å®Œæ•´è®­ç»ƒå¾ªç¯**:
```python
def dpo_train_step(model, ref_model, batch, beta=0.1):
    """
    DPOå•æ­¥è®­ç»ƒ
    """
    chosen_input_ids = batch['chosen_input_ids']
    rejected_input_ids = batch['rejected_input_ids']

    # ç­–ç•¥æ¨¡å‹å‰å‘ä¼ æ’­
    policy_chosen_logits = model(chosen_input_ids)
    policy_rejected_logits = model(rejected_input_ids)

    # å‚è€ƒæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
    with torch.no_grad():
        ref_chosen_logits = ref_model(chosen_input_ids)
        ref_rejected_logits = ref_model(rejected_input_ids)

    # è®¡ç®—logæ¦‚ç‡
    policy_chosen_logps = F.log_softmax(policy_chosen_logits, dim=-1)
    policy_rejected_logps = F.log_softmax(policy_rejected_logits, dim=-1)
    ref_chosen_logps = F.log_softmax(ref_chosen_logits, dim=-1)
    ref_rejected_logps = F.log_softmax(ref_rejected_logits, dim=-1)

    # è®¡ç®—DPOæŸå¤±
    loss, accuracy = dpo_loss(
        policy_chosen_logps, policy_rejected_logps,
        ref_chosen_logps, ref_rejected_logps,
        beta=beta
    )

    return loss, accuracy
```

---

### 4. å…¶ä»–å¯¹é½æ–¹æ³•

#### 4.1 ORPO (Odds Ratio Preference Optimization)

**åŸç†**: åœ¨SFTæŸå¤±åŸºç¡€ä¸Šæ·»åŠ åå¥½é¡¹

**æŸå¤±å‡½æ•°**:
```python
def orpo_loss(policy_chosen_logps, policy_rejected_logps, beta=0.1):
    """
    ORPOæŸå¤±: SFTæŸå¤± + åå¥½æŸå¤±
    """
    # SFTæŸå¤±ï¼ˆæ ‡å‡†è¯­è¨€å»ºæ¨¡æŸå¤±ï¼‰
    sft_loss = -(policy_chosen_logps.mean())

    # åå¥½æŸå¤±ï¼ˆodds ratioï¼‰
    log_odds = (policy_chosen_logps - policy_rejected_logps).exp()
    preference_loss = -log_odds.log().mean() * beta

    # æ€»æŸå¤±
    total_loss = sft_loss + preference_loss

    return total_loss
```

#### 4.2 Constitutional AI (CAI)

**åŸç†**: åŸºäºåŸåˆ™çš„è‡ªæˆ‘æ‰¹è¯„å’Œä¿®æ­£

**ä¸¤é˜¶æ®µ**:
```python
# é˜¶æ®µ1: æ‰¹è¯„
critique_prompt = f"""
æ ¹æ®ä»¥ä¸‹åŸåˆ™æ‰¹è¯„å›å¤:
åŸåˆ™: {constitution}
å›å¤: {response}

æ‰¹è¯„:
"""

# é˜¶æ®µ2: ä¿®æ­£
revision_prompt = f"""
æ ¹æ®æ‰¹è¯„ä¿®æ”¹å›å¤:
åŸå§‹å›å¤: {response}
æ‰¹è¯„: {critique}

ä¿®æ”¹åçš„å›å¤:
"""

# è¿­ä»£ä¼˜åŒ–
for _ in range(num_rounds):
    critique = model.generate(critique_prompt)
    revised_response = model.generate(revision_prompt)
```

---

## ğŸ”§ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: å®Œæ•´RLHFè®­ç»ƒæµç¨‹

```python
class RLHFTrainer:
    """å®Œæ•´çš„RLHFè®­ç»ƒå™¨"""

    def __init__(self, policy_model, ref_model, reward_model):
        self.policy = policy_model
        self.ref_model = ref_model
        self.reward_model = reward_model

        # PPOè¶…å‚æ•°
        self.clip_param = 0.2
        self.kl_coeff = 0.02

    def generate_responses(self, prompts):
        """ç”Ÿæˆå“åº”"""
        responses = []
        for prompt in prompts:
            response = self.policy.generate(prompt, max_length=256)
            responses.append(response)
        return responses

    def compute_rewards(self, prompts, responses):
        """è®¡ç®—å¥–åŠ±"""
        inputs = [f"{p}{r}" for p, r in zip(prompts, responses)]
        rewards = self.reward_model(inputs)
        return rewards

    def ppo_step(self, prompts, old_responses, old_log_probs):
        """PPOå•æ­¥æ›´æ–°"""
        # ç”Ÿæˆæ–°å“åº”
        new_responses = self.generate_responses(prompts)
        new_log_probs = self.policy.get_log_probs(prompts, new_responses)

        # è®¡ç®—å¥–åŠ±
        rewards = self.compute_rewards(prompts, new_responses)

        # è®¡ç®—ä¼˜åŠ¿
        advantages = compute_advantages(rewards, values)

        # è®¡ç®—KLæ•£åº¦
        ref_log_probs = self.ref_model.get_log_probs(prompts, new_responses)
        kl_div = kl_penalty(new_log_probs, ref_log_probs)

        # PPOæŸå¤±
        policy_loss = ppo_loss(
            new_log_probs, old_log_probs,
            advantages, self.clip_param
        )

        # æ€»æŸå¤±
        total_loss = policy_loss + self.kl_coeff * kl_div

        return total_loss

    def train(self, dataset, num_epochs=3):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        for epoch in range(num_epochs):
            for batch in dataset:
                # PPOæ›´æ–°
                loss = self.ppo_step(batch['prompts'], batch['responses'],
                                    batch['log_probs'])

                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

                print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
```

### æ¡ˆä¾‹2: DPOè®­ç»ƒå™¨

```python
class DPOTrainer:
    """DPOè®­ç»ƒå™¨"""

    def __init__(self, policy_model, ref_model, beta=0.1):
        self.policy = policy_model
        self.ref_model = ref_model
        self.beta = beta

    def train_step(self, batch):
        """DPOå•æ­¥è®­ç»ƒ"""
        # æå–chosenå’Œrejected
        chosen = batch['chosen']
        rejected = batch['rejected']

        # ç­–ç•¥æ¨¡å‹logæ¦‚ç‡
        policy_chosen_logps = self.policy.get_log_probs(chosen)
        policy_rejected_logps = self.policy.get_log_probs(rejected)

        # å‚è€ƒæ¨¡å‹logæ¦‚ç‡ï¼ˆæ— æ¢¯åº¦ï¼‰
        with torch.no_grad():
            ref_chosen_logps = self.ref_model.get_log_probs(chosen)
            ref_rejected_logps = self.ref_model.get_log_probs(rejected)

        # DPOæŸå¤±
        loss, accuracy = dpo_loss(
            policy_chosen_logps, policy_rejected_logps,
            ref_chosen_logps, ref_rejected_logps,
            self.beta
        )

        return loss, accuracy

    def train(self, dataset, num_epochs=3):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        optimizer = torch.optim.AdamW(self.policy.parameters(), lr=1e-5)

        for epoch in range(num_epochs):
            total_loss = 0
            total_acc = 0

            for batch in dataset:
                loss, acc = self.train_step(batch)

                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

                total_loss += loss.item()
                total_acc += acc.item()

            avg_loss = total_loss / len(dataset)
            avg_acc = total_acc / len(dataset)

            print(f"Epoch {epoch}: Loss={avg_loss:.4f}, Acc={avg_acc:.2%}")
```

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. åå¥½æ•°æ®æ”¶é›†

**è´¨é‡ä¼˜äºæ•°é‡**:
```python
# å¥½çš„åå¥½æ•°æ®ç¤ºä¾‹
prompt = "è§£é‡Šæ°”å€™å˜åŒ–çš„åŸå› "

chosen = """
æ°”å€™å˜åŒ–ä¸»è¦ç”±æ¸©å®¤æ°”ä½“æ’æ”¾å¼•èµ·ã€‚åŒ…æ‹¬:
1. äºŒæ°§åŒ–ç¢³ï¼šåŒ–çŸ³ç‡ƒæ–™ç‡ƒçƒ§
2. ç”²çƒ·ï¼šç•œç‰§ä¸šã€åƒåœ¾å¡«åŸ‹
3. æ°§åŒ–äºšæ°®ï¼šå†œä¸šæ´»åŠ¨
è¿™äº›æ°”ä½“åœ¨å¤§æ°”ä¸­å½¢æˆæ¸©å®¤æ•ˆåº”ï¼Œå¯¼è‡´å…¨çƒå˜æš–ã€‚
"""

rejected = """
å¤©æ°”å˜çƒ­æ˜¯å› ä¸ºå¤ªé˜³æ´»åŠ¨é¢‘ç¹ã€‚è¿˜æœ‰äººè¯´è¿™æ˜¯è‡ªç„¶ç°è±¡ï¼Œ
ä¸éœ€è¦æ‹…å¿ƒã€‚æˆ‘è§‰å¾—å¤§å®¶å¤ªå¤¸å¼ äº†ï¼Œå¤å¤©çƒ­å¾ˆæ­£å¸¸ã€‚
"""

# ç‰¹ç‚¹:
# - chosen: ç»“æ„æ¸…æ™°ã€äº‹å®å‡†ç¡®ã€æœ‰é€»è¾‘
# - rejected: ä¿¡æ¯é”™è¯¯ã€è§‚ç‚¹æ··ä¹±ã€ç¼ºä¹ä¾æ®
```

**æ•°æ®æ”¶é›†ç­–ç•¥**:
```python
# 1. ä»å¤šä¸ªæ¥æºæ”¶é›†
sources = [
    "human_annotations",      # äººå·¥æ ‡æ³¨ï¼ˆé«˜è´¨é‡ï¼‰
    "model_comparison",       # æ¨¡å‹ç”Ÿæˆå¯¹æ¯”
    "user_feedback",          # ç”¨æˆ·åé¦ˆï¼ˆçœŸå®åœºæ™¯ï¼‰
]

# 2. å¹³è¡¡éš¾åº¦åˆ†å¸ƒ
difficulty_levels = ["easy", "medium", "hard"]
for level in difficulty_levels:
    collect_data(difficulty=level)

# 3. è¦†ç›–å¤šç§ä»»åŠ¡
task_types = [
    "question_answering",
    "summarization",
    "creative_writing",
    "coding",
]
```

### 2. å¥–åŠ±æ¨¡å‹è®­ç»ƒæŠ€å·§

**æŠ€å·§1: é¢„è®­ç»ƒåˆå§‹åŒ–**
```python
# ä»SFTæ¨¡å‹åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹
reward_model = AutoModel.from_pretrained("sft-model-checkpoint")
reward_model.reward_head = nn.Linear(hidden_size, 1)
```

**æŠ€å·§2: æ•°æ®å¢å¼º**
```python
def augment_pair(chosen, rejected):
    """æ•°æ®å¢å¼º"""
    # åŒä¹‰è¯æ›¿æ¢
    augmented_chosen = synonym_replace(chosen)

    # å›è¯‘
    augmented_chosen = back_translate(chosen)

    # æ·»åŠ å™ªå£°
    augmented_rejected = add_noise(rejected)

    return augmented_chosen, augmented_rejected
```

**æŠ€å·§3: æŸå¤±åŠ æƒ**
```python
def weighted_reward_loss(reward_chosen, reward_rejected, margin):
    """
    åŠ æƒæŸå¤±ï¼Œå¼ºè°ƒé«˜ç½®ä¿¡åº¦æ ·æœ¬
    """
    diff = reward_chosen - reward_rejected
    weight = torch.sigmoid(diff / margin)
    loss = -weight * F.logsigmoid(diff).mean()
    return loss
```

### 3. PPOè®­ç»ƒç¨³å®šæ€§

**æŠ€å·§1: æ¢¯åº¦è£å‰ª**
```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**æŠ€å·§2: å­¦ä¹ ç‡è°ƒåº¦**
```python
# ä½™å¼¦é€€ç«
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=num_epochs, eta_min=1e-6
)
```

**æŠ€å·§3: å€¼å‡½æ•°å½’ä¸€åŒ–**
```python
# å½’ä¸€åŒ–ä¼˜åŠ¿å‡½æ•°
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
```

---

## ğŸ¯ å­¦ä¹ æ£€éªŒ

### å…³é”®é—®é¢˜

1. **å¯¹é½åŸºç¡€**:
   - ä»€ä¹ˆæ˜¯å¯¹é½é—®é¢˜ï¼Ÿä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ
   - RLHF vs DPOçš„åŒºåˆ«ï¼Ÿ
   - å¦‚ä½•æ”¶é›†é«˜è´¨é‡çš„åå¥½æ•°æ®ï¼Ÿ

2. **RLHFç®—æ³•**:
   - RLHFçš„ä¸‰ä¸ªé˜¶æ®µï¼Ÿ
   - å¥–åŠ±æ¨¡å‹å¦‚ä½•è®­ç»ƒï¼Ÿ
   - PPOçš„ç›®æ ‡å‡½æ•°ï¼Ÿ

3. **DPOç®—æ³•**:
   - DPOçš„æ ¸å¿ƒæ€æƒ³ï¼Ÿ
   - å¦‚ä½•ç›´æ¥ä¼˜åŒ–åå¥½ï¼Ÿ
   - DPOçš„ä¼˜åŠ¿å’Œå±€é™ï¼Ÿ

4. **å®è·µåº”ç”¨**:
   - å¦‚ä½•è®¾è®¡å®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹ï¼Ÿ
   - å¦‚ä½•æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Ÿ
   - å¦‚ä½•è¯„ä¼°å¯¹é½æ•ˆæœï¼Ÿ

### ä»£ç ç»ƒä¹ 

å®Œæˆ [examples.py](examples.py) ä¸­çš„ç»ƒä¹ é¢˜ã€‚

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**è®ºæ–‡**:
- "Training Language Models to Follow Instructions with Human Feedback" (InstructGPT)
- "Constitutional AI: Harmlessness from AI Feedback" (Anthropic)
- "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
- "Learning to Summarize with Human Feedback" (RLHFå¼€åˆ›æ€§å·¥ä½œ)

**ä»£ç å‚è€ƒ**:
- [CS336 Assignment 5](references/github/assignment5-alignment/)
- [Transformer Reinforcement Learning](https://github.com/lucidrains/trans-rlhf)
- [trl (Hugging Face)](https://github.com/huggingface/trl)

---

## âš ï¸ å¸¸è§é™·é˜±

1. **åå¥½æ•°æ®è´¨é‡**:
   - âŒ ä½¿ç”¨ä½è´¨é‡è‡ªåŠ¨ç”Ÿæˆçš„åå¥½å¯¹
   - âŒ æ ‡æ³¨ä¸ä¸€è‡´ï¼ˆä¸åŒæ ‡æ³¨å‘˜æ ‡å‡†ä¸åŒï¼‰
   - âœ… ä¸¥æ ¼çš„äººå·¥æ ‡æ³¨æµç¨‹
   - âœ… å®šæœŸéªŒè¯æ ‡æ³¨è´¨é‡

2. **å¥–åŠ±æ¨¡å‹è¿‡æ‹Ÿåˆ**:
   - âŒ åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒå¤ªå¤šepoch
   - âŒ å¥–åŠ±å€¼åˆ†å¸ƒå¼‚å¸¸ï¼ˆè¿‡å¤§æˆ–è¿‡å°ï¼‰
   - âœ… æ—©åœï¼ˆEarly Stoppingï¼‰
   - âœ… åœ¨éªŒè¯é›†ä¸Šç›‘æ§

3. **PPOè®­ç»ƒä¸ç¨³å®š**:
   - âŒ KLç³»æ•°å¤ªå¤§ï¼ˆç­–ç•¥æ›´æ–°å¤ªä¿å®ˆï¼‰
   - âŒ å­¦ä¹ ç‡å¤ªå¤§ï¼ˆç­–ç•¥å´©æºƒï¼‰
   - âœ… æ¸è¿›å¼å¢åŠ KLç³»æ•°
   - âœ… ä½¿ç”¨å‚è€ƒç­–ç•¥çº¦æŸ

4. **DPOå®ç°é”™è¯¯**:
   - âŒ å‚è€ƒæ¨¡å‹æ²¡æœ‰å†»ç»“
   - âŒ betaå‚æ•°è®¾ç½®ä¸å½“
   - âœ… å‚è€ƒæ¨¡å‹eval()æ¨¡å¼
   - âœ… betaåœ¨[0.1, 0.5]èŒƒå›´

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®ŒæˆAssignment 5åï¼Œä½ åº”è¯¥æŒæ¡ï¼š
- âœ… ç†è§£å¯¹é½é—®é¢˜çš„æ ¸å¿ƒæŒ‘æˆ˜
- âœ… æŒæ¡RLHFå®Œæ•´è®­ç»ƒæµç¨‹
- âœ… ç†è§£DPOçš„åŸç†å’Œå®ç°
- âœ… èƒ½å¤Ÿç‹¬ç«‹è®­ç»ƒå¯¹é½æ¨¡å‹

**æ¨èé¡¹ç›®**:
1. åœ¨å°æ¨¡å‹ä¸Šå®ç°å®Œæ•´RLHFæµç¨‹
2. å¯¹æ¯”RLHFå’ŒDPOçš„æ•ˆæœ
3. ç ”ç©¶Constitutional AIåœ¨ä¸­æ–‡åœºæ™¯çš„åº”ç”¨

**ä¸‹ä¸€æ­¥**: [Week 5: Agentæ¶æ„](../../Week5_Agents/README.md)
