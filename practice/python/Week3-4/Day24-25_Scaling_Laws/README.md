# Day 24-25: Scaling Laws - ç†è®ºä¸å®è·µ

> **å­¦ä¹ ç›®æ ‡**: ç†è§£Scaling LawsåŸç†ï¼ŒæŒæ¡æ¨¡å‹æ€§èƒ½é¢„æµ‹æ–¹æ³•ï¼Œå®ŒæˆCS336 Assignment 3æ ¸å¿ƒå†…å®¹
> **æ—¶é—´åˆ†é…**: 6å°æ—¶ï¼ˆç†è®º3h + å®è·µ3hï¼‰
> **éš¾åº¦**: â­â­â­â­
> **é‡è¦æ€§**: â­â­â­â­â­ (å¤§æ¨¡å‹è®­ç»ƒçš„æŒ‡å¯¼åŸåˆ™)

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. Scaling LawsåŸºç¡€

#### 1.1 ä»€ä¹ˆæ˜¯Scaling Lawsï¼Ÿ

**å®šä¹‰**: æè¿°æ¨¡å‹æ€§èƒ½éšè®¡ç®—èµ„æºã€æ•°æ®é‡ã€æ¨¡å‹è§„æ¨¡å˜åŒ–çš„è§„å¾‹

**æ ¸å¿ƒå‘ç°** (Kaplan et al., 2020; Chinchilla, 2022):

```
æ¨¡å‹æ€§èƒ½ï¼ˆLossï¼‰ä¸ä»¥ä¸‹å› ç´ å¹‚å¾‹ç›¸å…³:
- æ¨¡å‹å‚æ•°é‡ N (Model Size)
- è®­ç»ƒæ•°æ®é‡ D (Dataset Size)
- è®¡ç®—é‡ C (Compute)
```

**æ•°å­¦è¡¨è¾¾**:
```
L(N, D) = E + A/N^Î± + B/D^Î²

å…¶ä¸­:
- L: æœ€ç»ˆæŸå¤±
- N: æ¨¡å‹å‚æ•°é‡
- D: è®­ç»ƒæ•°æ®é‡ï¼ˆtokensï¼‰
- E, A, B, Î±, Î²: æ‹Ÿåˆå‚æ•°
```

#### 1.2 Chinchilla Scaling Laws

**å…³é”®ç»“è®º** (Hoffmann et al., 2022):

**è®¡ç®—æœ€ä¼˜**: ç»™å®šè®¡ç®—é¢„ç®—Cï¼Œæœ€ä¼˜çš„æ¨¡å‹å¤§å°å’Œæ•°æ®é‡æ»¡è¶³

```
N_opt âˆ C^(1/(Î±+Î²))
D_opt âˆ C^(1/(Î±+Î²))
```

**é‡è¦å‘ç°**:
- ä¹‹å‰çš„æ¨¡å‹**æ•°æ®è®­ç»ƒä¸è¶³**ï¼ˆunders optimizedï¼‰
- Chinchillaæ³•åˆ™: **Nå’ŒDåº”è¯¥åŒæ¯”ä¾‹å¢é•¿**

**ç¤ºä¾‹å¯¹æ¯”**:
```
GPT-3: 175Bå‚æ•°ï¼Œ300B tokens  (æ•°æ®ä¸è¶³)
Chinchillaæœ€ä¼˜: 70Bå‚æ•°ï¼Œ1.4T tokens  (è®¡ç®—æœ€ä¼˜)
```

---

### 2. æ¨¡å‹æ€§èƒ½é¢„æµ‹

#### 2.1 æŸå¤±é¢„æµ‹å…¬å¼

**Kapler Scaling Law** (2020):
```
L(N, D) = E(N) + A(N)/D^Î²(N)

å…¶ä¸­:
E(N) = E_âˆ + A/N^Î±  (å½“Dâ†’âˆæ—¶çš„æŸå¤±)
```

**Chinchillaæ”¹è¿›** (2022):
```
L(N, D) = E + A/N^Î± + B/D^Î²

æ›´ç®€æ´: åªéœ€6ä¸ªå‚æ•°ï¼ˆE, A, B, Î±, Î²ï¼‰
```

#### 2.2 æ‹ŸåˆScaling Laws

**æ­¥éª¤1**: æ”¶é›†è®­ç»ƒæ•°æ®
```python
experiments = [
    {"N": 1e8, "D": 1e9, "loss": 2.5},
    {"N": 5e8, "D": 5e9, "loss": 2.0},
    {"N": 1e9, "D": 1e10, "loss": 1.8},
    # ...
]
```

**æ­¥éª¤2**: æœ€å°äºŒä¹˜æ‹Ÿåˆ
```python
from scipy.optimize import curve_fit

def scaling_law(x, E, A, B, alpha, beta):
    N, D = x
    return E + A/N**alpha + B/D**beta

params, _ = curve_fit(
    scaling_law,
    (experiments["N"], experiments["D"]),
    experiments["loss"]
)
```

**æ­¥éª¤3**: é¢„æµ‹
```python
# é¢„æµ‹1Bå‚æ•°ã€10B tokensçš„æŸå¤±
predicted_loss = scaling_law((1e9, 1e10), *params)
```

---

### 3. è®¡ç®—æœ€ä¼˜è®­ç»ƒç­–ç•¥

#### 3.1 è®¡ç®—é‡å®šä¹‰

**è®­ç»ƒè®¡ç®—é‡** (FLOPs):
```
C â‰ˆ 6 Ã— N Ã— D

å…¶ä¸­:
- N: æ¨¡å‹å‚æ•°é‡
- D: è®­ç»ƒtokensæ•°
- 6: æ¯ä¸ªå‚æ•°çš„å‰å‘+åå‘è®¡ç®—ï¼ˆçº¦æ•°ï¼‰
```

**ç¤ºä¾‹**:
```
GPT-3 (175B):
N = 175e9
D = 300e9
C = 6 Ã— 175e9 Ã— 300e9 = 3.15e23 FLOPs
```

#### 3.2 æœ€ä¼˜åˆ†é…ç­–ç•¥

**é—®é¢˜**: ç»™å®šè®¡ç®—é¢„ç®—Cï¼Œå¦‚ä½•åˆ†é…Nå’ŒDï¼Ÿ

**Chinchillaæœ€ä¼˜è§£**:
```
N_opt = (C / 6)^(1/(Î±+Î²)) Ã— (AÎ±/BÎ²)^(Î²/(Î±+Î²))
D_opt = (C / 6)^(1/(Î±+Î²)) Ã— (BÎ²/AÎ±)^(Î±/(Î±+Î²))
```

**ç®€åŒ–** (å¯¹äºå…¸å‹å€¼Î±â‰ˆ0.35, Î²â‰ˆ0.37):
```
N_opt â‰ˆ 0.04 Ã— C^0.5
D_opt â‰ˆ 20 Ã— N_opt
```

**å®è·µ**:
```python
def compute_optimal_nd(compute_budget, A, B, alpha, beta):
    """è®¡ç®—æœ€ä¼˜çš„Nå’ŒD"""
    # å‡è®¾C = 6ND
    ratio = (A * alpha) / (B * beta)

    N_opt = (compute_budget / 6) ** (1/(alpha+beta)) * ratio ** (beta/(alpha+beta))
    D_opt = (compute_budget / 6) ** (1/(alpha+beta)) * (1/ratio) ** (alpha/(alpha+beta))

    return N_opt, D_opt
```

---

### 4. IsoFLOPsæ›²çº¿

#### 4.1 ä»€ä¹ˆæ˜¯IsoFLOPsï¼Ÿ

**å®šä¹‰**: åœ¨å›ºå®šè®¡ç®—é‡ä¸‹ï¼Œæ¨¡å‹å¤§å°Nä¸æ•°æ®é‡Dçš„æƒè¡¡æ›²çº¿

**ç¤ºä¾‹**:
```
å¯¹äºC = 10^22 FLOPs:
- æ–¹æ¡ˆ1: N=1B, D=1.67T (å¤§æ¨¡å‹ï¼Œå°‘æ•°æ®)
- æ–¹æ¡ˆ2: N=100M, D=16.7T (å°æ¨¡å‹ï¼Œå¤šæ•°æ®)
- æ–¹æ¡ˆ3: N=400M, D=4.2T (Chinchillaæœ€ä¼˜)
```

#### 4.2 ç»˜åˆ¶IsoFLOPs

```python
import matplotlib.pyplot as plt
import numpy as np

def plot_isoflops(compute_budget, A, B, alpha, beta):
    """ç»˜åˆ¶IsoFLOPsæ›²çº¿"""
    N_range = np.logspace(7, 10, 100)  # 10M - 10B

    # è®¡ç®—å¯¹åº”çš„D (C = 6ND)
    D_range = compute_budget / (6 * N_range)

    # è®¡ç®—æŸå¤±
    losses = E + A/N_range**alpha + B/D_range**beta

    # æ‰¾åˆ°æœ€ä¼˜
    optimal_idx = np.argmin(losses)
    N_opt = N_range[optimal_idx]
    D_opt = D_range[optimal_idx]

    plt.figure(figsize=(10, 6))
    plt.loglog(N_range, losses)
    plt.scatter([N_opt], [losses[optimal_idx]], c='red', s=100, label=f'Optimal: N={N_opt:.0e}, D={D_opt:.0e}')
    plt.xlabel('Model Size (N)')
    plt.ylabel('Loss')
    plt.title(f'IsoFLOPs Curve (C={compute_budget:.0e})')
    plt.legend()
    plt.grid(True)
    plt.show()
```

---

## ğŸ”§ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: æ‹ŸåˆScaling Laws

```python
import numpy as np
from scipy.optimize import curve_fit

class ScalingLawFitter:
    """Scaling Lawsæ‹Ÿåˆå™¨"""

    def __init__(self):
        self.params = None

    def scaling_law(self, x, E, A, B, alpha, beta):
        """Chinchilla scaling law"""
        N, D = x
        return E + A/N**alpha + B/D**beta

    def fit(self, experiments):
        """
        æ‹ŸåˆScaling Laws

        Args:
            experiments: [{"N": ..., "D": ..., "loss": ...}, ...]
        """
        N = np.array([e["N"] for e in experiments])
        D = np.array([e["D"] for e in experiments])
        loss = np.array([e["loss"] for e in experiments])

        # åˆå§‹çŒœæµ‹
        initial_guess = [1.8, 400, 400, 0.35, 0.37]

        # æ‹Ÿåˆ
        self.params, _ = curve_fit(
            self.scaling_law,
            (N, D),
            loss,
            p0=initial_guess,
            maxfev=10000
        )

        E, A, B, alpha, beta = self.params
        print(f"æ‹Ÿåˆå‚æ•°:")
        print(f"  E = {E:.4f}")
        print(f"  A = {A:.4f}")
        print(f"  B = {B:.4f}")
        print(f"  Î± = {alpha:.4f}")
        print(f"  Î² = {beta:.4f}")

    def predict(self, N, D):
        """é¢„æµ‹æŸå¤±"""
        if self.params is None:
            raise ValueError("æ¨¡å‹æœªæ‹Ÿåˆ")

        return self.scaling_law((N, D), *self.params)

    def compute_optimal(self, compute_budget):
        """è®¡ç®—æœ€ä¼˜Nå’ŒD"""
        if self.params is None:
            raise ValueError("æ¨¡å‹æœªæ‹Ÿåˆ")

        _, A, B, alpha, beta = self.params

        # æœ€ä¼˜è§£
        ratio = (A * alpha) / (B * beta)

        N_opt = (compute_budget / 6) ** (1/(alpha+beta)) * ratio ** (beta/(alpha+beta))
        D_opt = (compute_budget / 6) ** (1/(alpha+beta)) * (1/ratio) ** (alpha/(alpha+beta))

        return N_opt, D_opt

# ä½¿ç”¨
fitter = ScalingLawFitter()

# æ¨¡æ‹Ÿå®éªŒæ•°æ®
experiments = [
    {"N": 100e6, "D": 1e9, "loss": 3.2},
    {"N": 200e6, "D": 2e9, "loss": 2.9},
    {"N": 500e6, "D": 5e9, "loss": 2.5},
    {"N": 1e9, "D": 1e10, "loss": 2.2},
]

# æ‹Ÿåˆ
fitter.fit(experiments)

# é¢„æµ‹
predicted_loss = fitter.predict(N=1e9, D=1e10)
print(f"\né¢„æµ‹æŸå¤±: {predicted_loss:.4f}")

# è®¡ç®—æœ€ä¼˜é…ç½®
C = 1e22  # ç»™å®šè®¡ç®—é¢„ç®—
N_opt, D_opt = fitter.compute_optimal(C)
print(f"è®¡ç®—é¢„ç®—C={C:.0e}çš„æœ€ä¼˜é…ç½®:")
print(f"  N_opt = {N_opt:.0e}")
print(f"  D_opt = {D_opt:.0e}")
print(f"  é¢„æµ‹æŸå¤± = {fitter.predict(N_opt, D_opt):.4f}")
```

---

### æ¡ˆä¾‹2: åˆ†æGPT-3è®­ç»ƒ

```python
def analyze_gpt3():
    """åˆ†æGPT-3çš„è®­ç»ƒæ•ˆç‡"""

    # GPT-3é…ç½®
    N_gpt3 = 175e9  # 175Bå‚æ•°
    D_gpt3 = 300e9  # 300B tokens

    # è®¡ç®—è®¡ç®—é‡
    C_gpt3 = 6 * N_gpt3 * D_gpt3

    # Chinchillaæœ€ä¼˜é…ç½®ï¼ˆå‡è®¾Î±=0.35, Î²=0.37ï¼‰
    # N_opt â‰ˆ 0.04 Ã— C^0.5
    # D_opt â‰ˆ 20 Ã— N_opt

    N_opt = 0.04 * (C_gpt3 ** 0.5)
    D_opt = 20 * N_opt

    print("GPT-3 vs Chinchillaæœ€ä¼˜:")
    print(f"\nGPT-3:")
    print(f"  N = {N_gpt3:.0e} (175B)")
    print(f"  D = {D_gpt3:.0e} (300B tokens)")
    print(f"  C = {C_gpt3:.0e} FLOPs")

    print(f"\nChinchillaæœ€ä¼˜:")
    print(f"  N_opt = {N_opt:.0e} ({N_opt/1e9:.1f}B)")
    print(f"  D_opt = {D_opt:.0e} ({D_opt/1e12:.1f}T tokens)")
    print(f"  C = {6 * N_opt * D_opt:.0e} FLOPs (ç›¸åŒ)")

    # ä¼°è®¡æ€§èƒ½æå‡
    # å‡è®¾E=1.8, A=400, B=400, Î±=0.35, Î²=0.37
    E, A, B, alpha, beta = 1.8, 400, 400, 0.35, 0.37

    loss_gpt3 = E + A/N_gpt3**alpha + B/D_gpt3**beta
    loss_opt = E + A/N_opt**alpha + B/D_opt**beta

    print(f"\né¢„æµ‹æŸå¤±:")
    print(f"  GPT-3: {loss_gpt3:.4f}")
    print(f"  Chinchillaæœ€ä¼˜: {loss_opt:.4f}")
    print(f"  æå‡: {(loss_gpt3 - loss_opt)/loss_gpt3*100:.1f}%")

analyze_gpt3()
```

---

## ğŸ’¡ å®è·µæŠ€å·§

### 1. å®éªŒè®¾è®¡

**åŸåˆ™**:
1. **å¯¹æ•°é‡‡æ ·**: Nå’ŒDæŒ‰å¯¹æ•°é—´éš”é‡‡æ ·
   ```python
   N_values = np.logspace(7, 9, 5)  # 10M - 1B, 5ä¸ªç‚¹
   D_values = np.logspace(9, 11, 5)  # 1B - 100B, 5ä¸ªç‚¹
   ```

2. **è¦†ç›–èŒƒå›´**: è‡³å°‘2ä¸ªæ•°é‡çº§
   ```python
   # å¥½çš„è®¾è®¡
   N = [10M, 30M, 100M, 300M, 1B]

   # ä¸å¥½çš„è®¾è®¡ï¼ˆèŒƒå›´å¤ªçª„ï¼‰
   N = [100M, 110M, 120M, 130M, 140M]
   ```

3. **å¹³è¡¡é‡‡æ ·**: Nå’ŒDçš„å˜åŒ–ç‹¬ç«‹
   ```python
   experiments = []
   for N in N_values:
       for D in D_values:
           experiments.append({"N": N, "D": D})
   ```

### 2. æ•°æ®æ”¶é›†

**å…³é”®æŒ‡æ ‡**:
```python
def train_and_log(model, dataloader, epochs):
    """è®­ç»ƒå¹¶è®°å½•å…³é”®æŒ‡æ ‡"""
    for epoch in range(epochs):
        # è®­ç»ƒ
        train_one_epoch(model, dataloader)

        # è¯„ä¼°
        val_loss = evaluate(model, val_dataloader)

        # è®°å½•
        log = {
            "epoch": epoch,
            "N": count_parameters(model),
            "D": epochs * epoch * batch_size,
            "loss": val_loss,
            "compute": 6 * count_parameters(model) * epochs * epoch * batch_size
        }

        save_log(log)
```

### 3. é¢„æµ‹éªŒè¯

**éªŒè¯æ–¹æ³•**:
```python
# è®­ç»ƒé›†æ‹Ÿåˆ
fitter.fit(train_experiments)

# æµ‹è¯•é›†éªŒè¯
test_errors = []
for exp in test_experiments:
    predicted = fitter.predict(exp["N"], exp["D"])
    actual = exp["loss"]
    error = abs(predicted - actual) / actual
    test_errors.append(error)

print(f"å¹³å‡é¢„æµ‹è¯¯å·®: {np.mean(test_errors)*100:.1f}%")
```

---

## ğŸ“Š å®é™…åº”ç”¨

### åº”ç”¨1: é¢„ç®—è§„åˆ’

**åœºæ™¯**: è®¡ç®—é¢„ç®—ä¸º10^23 FLOPsï¼Œå¦‚ä½•è§„åˆ’æ¨¡å‹ï¼Ÿ

```python
def plan_model(compute_budget, target_loss=2.0):
    """è§„åˆ’æ¨¡å‹è®­ç»ƒ"""

    # å‡è®¾å·²æ‹Ÿåˆå‚æ•°
    E, A, B, alpha, beta = 1.8, 400, 400, 0.35, 0.37

    # è®¡ç®—æœ€ä¼˜é…ç½®
    fitter = ScalingLawFitter()
    fitter.params = (E, A, B, alpha, beta)

    N_opt, D_opt = fitter.compute_optimal(compute_budget)

    # é¢„æµ‹æŸå¤±
    predicted_loss = fitter.predict(N_opt, D_opt)

    print(f"è®¡ç®—é¢„ç®—: {compute_budget:.0e} FLOPs")
    print(f"æœ€ä¼˜é…ç½®:")
    print(f"  æ¨¡å‹å¤§å°: {N_opt/1e9:.2f}B å‚æ•°")
    print(f"  è®­ç»ƒæ•°æ®: {D_opt/1e12:.2f}T tokens")
    print(f"  é¢„æµ‹æŸå¤±: {predicted_loss:.4f}")

    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç›®æ ‡
    if predicted_loss <= target_loss:
        print(f"âœ… æ»¡è¶³ç›®æ ‡æŸå¤± {target_loss}")
    else:
        print(f"âŒ ä¸æ»¡è¶³ç›®æ ‡æŸå¤± {target_loss}")
        print(f"   éœ€è¦å¢åŠ è®¡ç®—é¢„ç®—åˆ°:")

        # åæ¨éœ€è¦çš„è®¡ç®—é‡
        # target_loss = E + A/N^Î± + B/D^Î²
        # ä¸” Nå’ŒDæ»¡è¶³æœ€ä¼˜å…³ç³»
        required_C = compute_required_budget(E, A, B, alpha, beta, target_loss)
        print(f"   {required_C:.0e} FLOPs")

    return N_opt, D_opt

# ä½¿ç”¨
plan_model(compute_budget=1e23, target_loss=2.0)
```

### åº”ç”¨2: æ¨¡å‹é€‰æ‹©

**å¯¹æ¯”ä¸åŒé…ç½®**:
```python
def compare_models(compute_budgets):
    """å¯¹æ¯”ä¸åŒé¢„ç®—ä¸‹çš„æœ€ä¼˜æ¨¡å‹"""

    budgets = [1e21, 1e22, 1e23, 1e24]

    print(f"{'é¢„ç®—(FLOPs)':<15} {'N(å‚æ•°)':<12} {'D(tokens)':<12} {'é¢„æµ‹æŸå¤±':<10}")
    print("-" * 50)

    for C in budgets:
        N_opt, D_opt = compute_optimal_nd(C, A, B, alpha, beta)
        loss = predict_loss(N_opt, D_opt, E, A, B, alpha, beta)

        print(f"{C:<15.0e} {N_opt/1e9:<12.2f} {D_opt/1e12:<12.2f} {loss:<10.4f}")

compare_models([1e21, 1e22, 1e23, 1e24])
```

---

## ğŸ¯ å­¦ä¹ æ£€éªŒ

### å…³é”®é—®é¢˜

1. **Scaling LawsåŸç†**:
   - ä»€ä¹ˆæ˜¯Scaling Lawsï¼Ÿ
   - Chinchillaæ³•åˆ™çš„æ ¸å¿ƒç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ
   - å¦‚ä½•æ‹ŸåˆScaling Lawsï¼Ÿ

2. **è®¡ç®—ä¼˜åŒ–**:
   - ç»™å®šè®¡ç®—é¢„ç®—ï¼Œå¦‚ä½•é€‰æ‹©æœ€ä¼˜Nå’ŒDï¼Ÿ
   - IsoFLOPsæ›²çº¿çš„å«ä¹‰ï¼Ÿ
   - å¦‚ä½•è¯„ä¼°è®­ç»ƒæ•ˆç‡ï¼Ÿ

3. **å®è·µåº”ç”¨**:
   - å¦‚ä½•é¢„æµ‹æ¨¡å‹æ€§èƒ½ï¼Ÿ
   - å¦‚ä½•è§„åˆ’è®­ç»ƒé¢„ç®—ï¼Ÿ
   - GPT-3è®­ç»ƒå“ªé‡Œå¯ä»¥æ”¹è¿›ï¼Ÿ

### ä»£ç ç»ƒä¹ 

å®Œæˆ [examples.py](examples.py) ä¸­çš„ç»ƒä¹ é¢˜ã€‚

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**è®ºæ–‡**:
- "Scaling Laws for Neural Language Models" (Kaplan et al., 2020)
- "Training Compute-Optimal Large Language Models" (Hoffmann et al., 2022) - Chinchilla

**ä»£ç **:
- [CS336 Assignment 3: Scaling](references/github/assignment3-scaling/)

---

**ä¸‹ä¸€æ­¥**: [Day 26-28: æ•°æ®å¤„ç†ä¸RAGè¿›é˜¶](../Day26-28_Data_Pipeline_RAG/README.md)
