# Day 26-27: Assignment 4 - Data Processing

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡æ•°æ®å¤„ç†ç®¡é“ï¼Œç†è§£å»é‡ç®—æ³•ï¼Œå®ç°ç”Ÿäº§çº§æ•°æ®æ¸…æ´—æµç¨‹
> **æ—¶é—´åˆ†é…**: 6å°æ—¶ï¼ˆç†è®º2h + å®è·µ4hï¼‰
> **éš¾åº¦**: â­â­â­â­
> **é‡è¦æ€§**: â­â­â­â­â­ (LLMåº”ç”¨çš„æ•°æ®è´¨é‡å…³é”®)
> **æ¥æº**: CS336 Assignment 4 - Data

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. æ•°æ®å¤„ç†ç®¡é“æ¦‚è¿°

**å®Œæ•´æµç¨‹**:
```
åŸå§‹æ•°æ® â†’ æ¸…æ´— â†’ å»é‡ â†’ åˆ†å— â†’ ç´¢å¼•
   â†“         â†“      â†“      â†“      â†“
 HTML/æ–‡æœ¬  çº¯æ–‡æœ¬  å”¯ä¸€æ–‡æ¡£  chunks  å‘é‡DB
```

**ä¸ºä»€ä¹ˆé‡è¦**:
- **æ•°æ®è´¨é‡**ç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ï¼ˆGarbage In, Garbage Outï¼‰
- RAGç³»ç»Ÿçš„æ£€ç´¢è´¨é‡80%å–å†³äºæ•°æ®å¤„ç†
- è®­ç»ƒæ•°æ®å»é‡å¯ä»¥æå‡è®­ç»ƒæ•ˆç‡

---

### 2. æ–‡æœ¬å¤„ç†

#### 2.1 HTMLè½¬æ–‡æœ¬

**é—®é¢˜**: ç½‘é¡µæ•°æ®åŒ…å«å¤§é‡å™ªå£°ï¼ˆHTMLæ ‡ç­¾ã€CSSã€JavaScriptï¼‰

**å·¥å…·å¯¹æ¯”**:
| å·¥å…· | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **BeautifulSoup** | ç®€å•æ˜“ç”¨ | éœ€è¦æ‰‹åŠ¨å¤„ç† | ç®€å•HTML |
| **trafilatura** | è‡ªåŠ¨æå–æ­£æ–‡ | ä¾èµ–é‡ | ç”Ÿäº§ç¯å¢ƒ |
| **unstructured** | æ”¯æŒå¤æ‚å¸ƒå±€ | æ…¢ | å¤šæ¨¡æ€æ–‡æ¡£ |

**æœ€ä½³å®è·µ**:
```python
import trafilatura

def html_to_text(html_content):
    """ä½¿ç”¨trafilaturaæå–æ­£æ–‡"""
    return trafilatura.extract(
        html_content,
        include_comments=False,
        include_tables=True,
        no_fallback=False
    )
```

#### 2.2 æ–‡æœ¬æ¸…æ´—

**æ¸…æ´—æ­¥éª¤**:
1. **å»é™¤å¤šä½™ç©ºç™½**: è¿ç»­ç©ºæ ¼ã€æ¢è¡Œç¬¦
2. **ç‰¹æ®Šå­—ç¬¦å¤„ç†**: Unicodeæ ‡å‡†åŒ–
3. **å»é™¤å¹¿å‘Š/å¯¼èˆªæ **: åŸºäºè§„åˆ™æˆ–ML
4. **è¯­è¨€æ£€æµ‹**: è¿‡æ»¤éç›®æ ‡è¯­è¨€

```python
import re
import unicodedata

def clean_text(text):
    """æ–‡æœ¬æ¸…æ´—"""
    # 1. Unicodeæ ‡å‡†åŒ–
    text = unicodedata.normalize('NFKC', text)

    # 2. å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)

    # 3. å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ ‡ç‚¹ï¼‰
    text = re.sub(r'[^\u4e00-\u9fff\u0041-\u007a\u0020-\u007e\uff0a\uff1f\uff08\uff09]', '', text)

    # 4. å»é™¤è¿‡çŸ­è¡Œ
    lines = [l for l in text.split('\n') if len(l.strip()) > 10]

    return '\n'.join(lines)
```

---

### 3. å»é‡ç®—æ³•

#### 3.1 ç²¾ç¡®å»é‡

**æ–¹æ³•**: æ–‡æœ¬å®Œå…¨ç›¸åŒæˆ–MD5å“ˆå¸Œç›¸åŒ

```python
import hashlib

def exact_deduplicate(documents):
    """ç²¾ç¡®å»é‡"""
    seen = set()
    unique_docs = []

    for doc in documents:
        # è®¡ç®—MD5å“ˆå¸Œ
        doc_hash = hashlib.md5(doc['text'].encode()).hexdigest()

        if doc_hash not in seen:
            seen.add(doc_hash)
            unique_docs.append(doc)

    return unique_docs
```

**é€‚ç”¨åœºæ™¯**:
- âœ… ç›¸åŒæ¥æºçš„æ•°æ®
- âœ… æ˜æ˜¾çš„é‡å¤å†…å®¹
- âŒ è¯­ä¹‰ç›¸ä¼¼ä½†æ–‡æœ¬ä¸åŒ

#### 3.2 MinHash LSHï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰

**åŸç†**:
1. å°†æ–‡æ¡£è½¬æ¢ä¸ºShingleé›†åˆï¼ˆè¿ç»­kä¸ªè¯ï¼‰
2. è®¡ç®—MinHashç­¾åï¼ˆå›ºå®šé•¿åº¦ï¼‰
3. ä½¿ç”¨LSHï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰å¿«é€ŸæŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£

**ä¼˜åŠ¿**:
- æ¯”å¯¹æ¯”å¿«1000å€
- å¯è°ƒèŠ‚ç›¸ä¼¼åº¦é˜ˆå€¼
- é€‚åˆå¤§è§„æ¨¡æ•°æ®é›†

**å®ç°**:
```python
from datasketch import MinHashLSH, MinHash

def minhash_deduplicate(documents, threshold=0.8):
    """ä½¿ç”¨MinHash LSHå»é‡"""

    # 1. åˆ›å»ºLSHç´¢å¼•
    lsh = MinHashLSH(threshold=threshold, num_perm=128)

    # 2. ä¸ºæ¯ä¸ªæ–‡æ¡£è®¡ç®—MinHash
    minhashes = {}
    for idx, doc in enumerate(documents):
        # åˆ†è¯
        words = doc['text'].split()

        # è®¡ç®—MinHashç­¾å
        m = MinHash(num_perm=128)
        for word in words:
            m.update(word.encode())

        # æ·»åŠ åˆ°ç´¢å¼•
        lsh.insert(idx, m)
        minhashes[idx] = m

    # 3. æŸ¥æ‰¾é‡å¤
    duplicates = set()
    for idx in range(len(documents)):
        # æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£
        result = lsh.query(minhashes[idx])
        for similar_idx in result:
            if similar_idx != idx and similar_idx not in duplicates:
                print(f"æ–‡æ¡£{idx}ä¸æ–‡æ¡£{similar_idx}ç›¸ä¼¼")
                duplicates.add(similar_idx)

    # 4. è¿”å›å»é‡åçš„æ–‡æ¡£
    return [doc for idx, doc in enumerate(documents) if idx not in duplicates]
```

#### 3.3 SimHashï¼ˆç›¸ä¼¼åº¦å“ˆå¸Œï¼‰

**åŸç†**:
1. å°†æ–‡æ¡£è½¬æ¢ä¸ºè¯å‘é‡
2. è®¡ç®—simhashæŒ‡çº¹ï¼ˆå›ºå®šé•¿åº¦ï¼‰
3. æ¯”è¾ƒæ±‰æ˜è·ç¦»

**é€‚ç”¨åœºæ™¯**:
- éœ€è¦å¿«é€Ÿå»é‡
- ç›¸ä¼¼åº¦é˜ˆå€¼å›ºå®š
- ä¸­æ–‡æ–‡æœ¬

---

### 4. å†…å®¹è¿‡æ»¤

#### 4.1 è´¨é‡è¯„åˆ†

**è¯„åˆ†æŒ‡æ ‡**:
1. **æ–‡æœ¬é•¿åº¦**: è¿‡çŸ­çš„æ–‡æ¡£è´¨é‡ä½
2. **å¥å­æ•°é‡**: å¥å­å¤ªå°‘å¯èƒ½ä¸å®Œæ•´
3. **ç‰¹æ®Šè¯æ¯”ä¾‹**: è¿‡å¤š"ç‚¹å‡»è¿™é‡Œ"ç­‰å¹¿å‘Šè¯
4. **æ ‡ç‚¹ç¬¦å·æ¯”ä¾‹**: è¿‡å°‘æ ‡ç‚¹å¯èƒ½æ˜¯å™ªå£°

```python
def quality_score(text):
    """è®¡ç®—æ–‡æ¡£è´¨é‡åˆ†æ•°ï¼ˆ0-1ï¼‰"""
    score = 0.0

    # 1. é•¿åº¦è¯„åˆ†
    length = len(text)
    if 100 <= length <= 10000:
        score += 0.3
    elif length > 10000:
        score += 0.2

    # 2. å¥å­æ•°é‡
    sentences = text.split('ã€‚')
    if 3 <= len(sentences) <= 100:
        score += 0.3

    # 3. æ ‡ç‚¹ç¬¦å·æ¯”ä¾‹
    punctuation_ratio = sum(1 for c in text if c in 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š') / len(text)
    if 0.02 <= punctuation_ratio <= 0.15:
        score += 0.2

    # 4. å¹¿å‘Šè¯æ£€æµ‹
    spam_keywords = ['ç‚¹å‡»', 'å¹¿å‘Š', 'æ¨å¹¿']
    if not any(kw in text for kw in spam_keywords):
        score += 0.2

    return score
```

#### 4.2 æœ‰å®³å†…å®¹è¿‡æ»¤

**æ–¹æ³•**:
1. **å…³é”®è¯è¿‡æ»¤**: é»‘åå•è¯æ±‡
2. **æ­£åˆ™è¡¨è¾¾å¼**: åŒ¹é…æœ‰å®³æ¨¡å¼
3. **MLæ¨¡å‹**: ä½¿ç”¨åˆ†ç±»å™¨è¯†åˆ«

```python
def filter_harmful_content(texts):
    """è¿‡æ»¤æœ‰å®³å†…å®¹"""
    harmful_keywords = [
        'æš´åŠ›', 'è‰²æƒ…', 'èµŒåš',
        # ... æ›´å¤šå…³é”®è¯
    ]

    filtered = []
    for text in texts:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰å®³å…³é”®è¯
        is_harmful = any(kw in text for kw in harmful_keywords)

        if not is_harmful:
            filtered.append(text)

    return filtered
```

---

## ğŸ”§ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: å®Œæ•´æ•°æ®å¤„ç†ç®¡é“

```python
class DataPipeline:
    """å®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“"""

    def __init__(self):
        self.documents = []

    def load_html(self, html_files):
        """åŠ è½½HTMLæ–‡ä»¶"""
        from trafilatura import extract

        for html_file in html_files:
            with open(html_file, 'r', encoding='utf-8') as f:
                html = f.read()

            # æå–æ­£æ–‡
            text = extract(html)

            self.documents.append({
                'source': html_file,
                'text': text
            })

    def clean(self):
        """æ¸…æ´—æ–‡æœ¬"""
        for doc in self.documents:
            doc['text'] = clean_text(doc['text'])

    def deduplicate(self, method='minhash'):
        """å»é‡"""
        if method == 'exact':
            self.documents = exact_deduplicate(self.documents)
        elif method == 'minhash':
            self.documents = minhash_deduplicate(self.documents, threshold=0.8)

    def filter_quality(self, min_score=0.6):
        """è´¨é‡è¿‡æ»¤"""
        filtered = []
        for doc in self.documents:
            score = quality_score(doc['text'])
            if score >= min_score:
                doc['quality_score'] = score
                filtered.append(doc)

        self.documents = filtered

    def process(self, html_files):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        print(f"åŠ è½½æ–‡æ¡£: {len(html_files)}ä¸ªæ–‡ä»¶")
        self.load_html(html_files)

        print(f"æ¸…æ´—å: {len(self.documents)}ä¸ªæ–‡æ¡£")
        self.clean()

        print(f"å»é‡å‰: {len(self.documents)}ä¸ªæ–‡æ¡£")
        self.deduplicate(method='minhash')
        print(f"å»é‡å: {len(self.documents)}ä¸ªæ–‡æ¡£")

        print(f"è´¨é‡è¿‡æ»¤å‰: {len(self.documents)}ä¸ªæ–‡æ¡£")
        self.filter_quality(min_score=0.6)
        print(f"è´¨é‡è¿‡æ»¤å: {len(self.documents)}ä¸ªæ–‡æ¡£")

        return self.documents

# ä½¿ç”¨
pipeline = DataPipeline()
html_files = ['data/file1.html', 'data/file2.html']
documents = pipeline.process(html_files)
```

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
def batch_process(html_files, batch_size=100):
    """æ‰¹é‡å¤„ç†HTMLæ–‡ä»¶"""
    results = []

    for i in range(0, len(html_files), batch_size):
        batch = html_files[i:i+batch_size]

        # å¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor() as executor:
            futures = [executor.submit(process_html, f) for f in batch]
            results.extend([f.result() for f in futures])

    return results
```

### 2. å¢é‡å»é‡

```python
class IncrementalDeduplicator:
    """å¢é‡å»é‡å™¨"""

    def __init__(self, threshold=0.8):
        self.lsh = MinHashLSH(threshold=threshold, num_perm=128)
        self.processed_count = 0

    def add_documents(self, new_docs):
        """æ·»åŠ æ–°æ–‡æ¡£"""
        for idx, doc in enumerate(new_docs):
            # è®¡ç®—MinHash
            words = doc['text'].split()
            m = MinHash(num_perm=128)
            for word in words:
                m.update(word.encode())

            # æ£€æŸ¥æ˜¯å¦é‡å¤
            duplicates = self.lsh.query(m)

            if not duplicates:
                # ä¸é‡å¤ï¼Œæ·»åŠ åˆ°ç´¢å¼•
                global_idx = self.processed_count + idx
                self.lsh.insert(global_idx, m)
                self.processed_count += 1
                yield doc
```

---

## ğŸ¯ å­¦ä¹ æ£€éªŒ

### å…³é”®é—®é¢˜

1. **æ•°æ®å¤„ç†**:
   - ä¸ºä»€ä¹ˆæ•°æ®æ¸…æ´—é‡è¦ï¼Ÿ
   - å¦‚ä½•é€‰æ‹©åˆé€‚çš„å»é‡æ–¹æ³•ï¼Ÿ
   - è´¨é‡è¯„åˆ†çš„æŒ‡æ ‡æœ‰å“ªäº›ï¼Ÿ

2. **å»é‡ç®—æ³•**:
   - MinHash LSHçš„åŸç†ï¼Ÿ
   - ç²¾ç¡®å»é‡vsæ¨¡ç³Šå»é‡ï¼Ÿ
   - å¦‚ä½•è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Ÿ

3. **å®è·µåº”ç”¨**:
   - å¦‚ä½•æ„å»ºç”Ÿäº§çº§æ•°æ®ç®¡é“ï¼Ÿ
   - å¦‚ä½•å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Ÿ
   - å¦‚ä½•ç›‘æ§æ•°æ®è´¨é‡ï¼Ÿ

### ä»£ç ç»ƒä¹ 

å®Œæˆ [examples.py](examples.py) ä¸­çš„ç»ƒä¹ é¢˜ã€‚

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**è®ºæ–‡**:
- "Text ë´ì „ìë§ì„ ìœ„í•œ í•œêµ­ì–´ BPE ëª¨ë¸" (BPEè®ºæ–‡)
- "MapReduce: Simplified Data Processing on Large Clusters"

**ä»£ç å‚è€ƒ**:
- [CS336 Assignment 4](references/github/assignment4-data/)
- [trafilatura](https://github.com/adbartra/trafilatura)
- [datasketch](https://github.com/boundedregression/datasketch)

---

## âš ï¸ å¸¸è§é™·é˜±

1. **è¿‡åº¦æ¸…æ´—**:
   - âŒ åˆ é™¤äº†æœ‰ç”¨ä¿¡æ¯
   - âœ… ä¿ç•™åŸå§‹æ–‡æ¡£ï¼Œç”Ÿæˆæ¸…æ´—åçš„å‰¯æœ¬

2. **å»é‡é˜ˆå€¼è®¾ç½®**:
   - âŒ é˜ˆå€¼å¤ªé«˜ï¼ˆ0.95ï¼‰å¯¼è‡´é‡å¤æœªå»é™¤
   - âŒ é˜ˆå€¼å¤ªä½ï¼ˆ0.5ï¼‰å¯¼è‡´è¯¯åˆ 
   - âœ… å…¸å‹å€¼ï¼š0.7-0.85

3. **å†…å­˜ç®¡ç†**:
   - âŒ ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡æ¡£
   - âœ… ä½¿ç”¨ç”Ÿæˆå™¨å’Œæ‰¹é‡å¤„ç†

4. **ä¸­æ–‡æ–‡æœ¬å¤„ç†**:
   - âŒ ä½¿ç”¨è‹±æ–‡åˆ†è¯å™¨
   - âœ… ä½¿ç”¨jiebaç­‰ä¸­æ–‡åˆ†è¯å·¥å…·

---

**ä¸‹ä¸€æ­¥**: [Day 28: RAGè¿›é˜¶æŠ€å·§](../Day28_RAG_Advanced/README.md)
