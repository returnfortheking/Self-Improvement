# Day 19: ç³»ç»Ÿä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡PyTorchæ€§èƒ½åˆ†æå·¥å…·ï¼Œå­¦ä¼šè¯†åˆ«å’Œè§£å†³è®­ç»ƒç“¶é¢ˆï¼Œä¼˜åŒ–åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½
> **æ—¶é—´åˆ†é…**: 6å°æ—¶ï¼ˆç†è®º2h + å®è·µ4hï¼‰
> **éš¾åº¦**: â­â­â­â­
> **é‡è¦æ€§**: â­â­â­â­â­ (ç”Ÿäº§ç¯å¢ƒå¿…å¤‡æŠ€èƒ½)

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. æ€§èƒ½åˆ†ææ¡†æ¶

è®­ç»ƒæ€§èƒ½çš„ä¸‰ä¸ªç»´åº¦ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è®­ç»ƒæ€§èƒ½ = è®¡ç®— + æ•°æ®åŠ è½½ + é€šä¿¡ (åˆ†å¸ƒå¼)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è®¡ç®—: å‰å‘ä¼ æ’­ + åå‘ä¼ æ’­ + ä¼˜åŒ–å™¨æ›´æ–°          â”‚
â”‚  æ•°æ®: DataLoader (CPUâ†’GPUä¼ è¾“)                 â”‚
â”‚  é€šä¿¡: æ¢¯åº¦åŒæ­¥ (AllReduce)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜åŒ–ç›®æ ‡**:
- âœ… æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ï¼ˆ>90%ï¼‰
- âœ… æœ€å°åŒ–CPUç­‰å¾…æ—¶é—´
- âœ… å‡å°‘é€šä¿¡å¼€é”€

---

### 2. PyTorch Profileræ·±åº¦è§£æ

#### 2.1 ProfileråŸºç¡€

**PyTorch Profiler** æ˜¯æ€§èƒ½åˆ†æçš„åˆ©å™¨ï¼š

```python
from torch.profiler import profile, ProfilerActivity, record_function

with profile(
    activities=[
        ProfilerActivity.CPU,      # CPUæ´»åŠ¨
        ProfilerActivity.CUDA,     # GPUæ´»åŠ¨
    ],
    record_shapes=True,            # è®°å½•tensor shapes
    profile_memory=True,           # åˆ†æå†…å­˜ä½¿ç”¨
    with_stack=True,               # è®°å½•è°ƒç”¨æ ˆ
) as prof:
    # è®­ç»ƒä»£ç 
    for batch in dataloader:
        output = model(batch)
        loss.backward()
        optimizer.step()

# æ‰“å°åˆ†æç»“æœ
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

#### 2.2 å…³é”®æŒ‡æ ‡è§£è¯»

**è¾“å‡ºè¡¨æ ¼åˆ—å«ä¹‰**:

| åˆ—å | å«ä¹‰ | ä¼˜åŒ–ç›®æ ‡ |
|------|------|----------|
| **Name** | æ“ä½œåç§° | - |
| **Self CUDA time** | è¯¥æ“ä½œè‡ªèº«GPUæ—¶é—´ | è¶ŠçŸ­è¶Šå¥½ |
| **CUDA time total** | åŒ…å«å­æ“ä½œçš„æ€»æ—¶é—´ | è¯†åˆ«ç“¶é¢ˆ |
| **Self CPU time** | è¯¥æ“ä½œCPUæ—¶é—´ | CPUåˆ©ç”¨ç‡ |
| **CPU time total** | æ€»CPUæ—¶é—´ | - |
| **Number of calls** | è°ƒç”¨æ¬¡æ•° | å‡å°‘ä¸å¿…è¦è°ƒç”¨ |

**ç¤ºä¾‹è¾“å‡º**:
```
-------------------------------------------------------
Name                   Self CUDA    CUDA time total
-------------------------------------------------------
aten::conv2d                  10.50ms           12.30ms
aten::convolution_backward    8.20ms           15.40ms
aten::relu                    0.05ms            0.05ms
ncclAllReduce                15.00ms           15.00ms  â† é€šä¿¡ç“¶é¢ˆ
-------------------------------------------------------
```

---

### 3. æ•°æ®åŠ è½½ä¼˜åŒ–

#### 3.1 DataLoaderç“¶é¢ˆåˆ†æ

**é—®é¢˜**: GPUç­‰å¾…æ•°æ®ï¼ˆGPUç©ºé—²ï¼‰

**è¯Šæ–­**:
```python
# åœ¨Profilerè¾“å‡ºä¸­æŸ¥æ‰¾
- DataLoaderè¿­ä»£: åº”è¯¥<5ms
- CPUâ†’GPUä¼ è¾“: pin_memoryååº”è¯¥<1ms
```

**ä¼˜åŒ–å‚æ•°**:

| å‚æ•° | é»˜è®¤å€¼ | ä¼˜åŒ–å»ºè®® | æ•ˆæœ |
|------|--------|----------|------|
| **num_workers** | 0 | 4-8ï¼ˆCPUæ ¸å¿ƒæ•°çš„ä¸€åŠï¼‰ | â­â­â­â­â­ |
| **pin_memory** | False | Trueï¼ˆè®­ç»ƒæ—¶ï¼‰ | â­â­â­â­ |
| **prefetch_factor** | 2 | 2-4 | â­â­â­ |
| **persistent_workers** | False | Trueï¼ˆå¤§æ•°æ®é›†ï¼‰ | â­â­ |

**æœ€ä½³é…ç½®**:
```python
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=8,              # å¹¶è¡Œæ•°æ®åŠ è½½
    pin_memory=True,             # é”é¡µå†…å­˜ï¼ŒåŠ é€ŸCPUâ†’GPUä¼ è¾“
    prefetch_factor=4,           # é¢„å–4ä¸ªbatch
    persistent_workers=True,     # ä¿æŒworkerè¿›ç¨‹
    drop_last=True               # ä¸¢å¼ƒæœ€åä¸å®Œæ•´batch
)
```

#### 3.2 è‡ªå®šä¹‰Collateä¼˜åŒ–

**é—®é¢˜**: é»˜è®¤collate_fnæ…¢

**ä¼˜åŒ–**:
```python
def custom_collate_fn(batch):
    """ä¼˜åŒ–çš„batch collateå‡½æ•°"""
    # ä½¿ç”¨torch.stackè€Œä¸æ˜¯å¾ªç¯
    images = torch.stack([item[0] for item in batch])
    labels = torch.tensor([item[1] for item in batch])

    # é¢„å…ˆè½¬ç§»åˆ°GPUï¼ˆå¦‚æœä½¿ç”¨pin_memoryï¼Œè¿™ä¸€æ­¥ä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼‰
    return images, labels

dataloader = DataLoader(
    dataset,
    batch_size=32,
    collate_fn=custom_collate_fn
)
```

---

### 4. è®¡ç®—ä¼˜åŒ–

#### 4.1 æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰

**åŸç†**:
```
FP32: ç²¾åº¦é«˜ï¼Œè®¡ç®—æ…¢ï¼Œæ˜¾å­˜å¤§
FP16: ç²¾åº¦è¾ƒä½ï¼Œè®¡ç®—å¿«ï¼Œæ˜¾å­˜å°
BF16: å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦ï¼ˆæ¨èï¼‰
```

**å®ç°**:
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    # å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰
    with autocast(dtype=torch.bfloat16):  # æˆ–torch.float16
        output = model(batch)
        loss = criterion(output, target)

    # åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨å¤„ç†ç¼©æ”¾ï¼‰
    scaler.scale(loss).backward()

    # æ¢¯åº¦è£å‰ªï¼ˆé‡è¦ï¼ï¼‰
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    # æ›´æ–°å‚æ•°
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

**æ€§èƒ½æå‡**:
- é€Ÿåº¦: 1.5-3xåŠ é€Ÿ
- æ˜¾å­˜: ~40%èŠ‚çœ
- ç²¾åº¦: å‡ ä¹æ— æŸï¼ˆä½¿ç”¨BF16ï¼‰

#### 4.2 æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰

**é—®é¢˜**: æ·±å±‚ç½‘ç»œçš„ä¸­é—´æ¿€æ´»å ç”¨å¤§é‡æ˜¾å­˜

**è§£å†³æ–¹æ¡ˆ**: åªä¿å­˜éƒ¨åˆ†æ¿€æ´»ï¼Œåä¼ æ—¶é‡æ–°è®¡ç®—

```python
from torch.utils.checkpoint import checkpoint

class CheckpointedTransformer(nn.Module):
    def __init__(self, d_model, n_layers):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerBlock(d_model) for _ in range(n_layers)
        ])

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            # æ¯2å±‚checkpointä¸€æ¬¡
            if i % 2 == 0 and i > 0:
                x = checkpoint(layer, x)  # é‡æ–°è®¡ç®—
            else:
                x = layer(x)
        return x
```

**æ•ˆæœ**:
- æ˜¾å­˜èŠ‚çœ: 30-50%
- é€Ÿåº¦ä»£ä»·: +20-30%è®¡ç®—æ—¶é—´

**é€‚ç”¨åœºæ™¯**:
- âœ… æ¨¡å‹å¤ªå¤§ï¼Œæ˜¾å­˜ä¸è¶³
- âœ… æ„¿æ„ç”¨æ—¶é—´æ¢ç©ºé—´

#### 4.3 ç®—å­èåˆï¼ˆOperator Fusionï¼‰

**åŸç†**: åˆå¹¶å¤šä¸ªå°æ“ä½œä¸ºä¸€ä¸ªkernel

```python
# æœªä¼˜åŒ–ï¼ˆå¤šæ¬¡kernel launchï¼‰
def forward(x):
    x = layer_norm(x)
    x = activation(x)
    x = linear(x)
    return x

# èåˆï¼ˆä¸€æ¬¡kernel launchï¼‰
def forward_fused(x):
    return fused_layer_norm_activation_linear(x)
```

**PyTorch JIT**:
```python
@torch.jit.script
def fused_function(x, weight, bias):
    x = torch.layer_norm(x)
    x = torch.relu(x)
    x = torch.linear(x, weight, bias)
    return x
```

---

### 5. åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–

#### 5.1 DDPé€šä¿¡ä¼˜åŒ–

**é€šä¿¡å¼€é”€åˆ†æ**:
```python
# åœ¨Profilerä¸­æŸ¥æ‰¾
ncclAllReduce           # æ¢¯åº¦åŒæ­¥
ncclBroadcast           # å‚æ•°å¹¿æ’­
```

**ä¼˜åŒ–ç­–ç•¥**:

1. **Gradient Bucketingè°ƒæ•´**:
   ```python
   model = DDP(
       model,
       bucket_cap_mb=25,  # å¢å¤§bucketå‡å°‘é€šä¿¡æ¬¡æ•°
   )
   ```

2. **è·³è¿‡unusedå‚æ•°åŒæ­¥**:
   ```python
   model = DDP(
       model,
       find_unused_parameters=False,  # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä½¿ç”¨
   )
   ```

3. **é€šä¿¡ä¸è®¡ç®—é‡å **:
   ```python
   # DDPè‡ªåŠ¨å®ç°ï¼Œä½†å¯ä»¥é€šè¿‡è°ƒæ•´bucketå¤§å°ä¼˜åŒ–
   ```

#### 5.2 å¤šGPUæ€§èƒ½æ‰©å±•æ€§

**æ‰©å±•æ•ˆç‡**:
```
åŠ é€Ÿæ¯” = å•GPUæ—¶é—´ / N GPUæ—¶é—´
æ•ˆç‡ = åŠ é€Ÿæ¯” / N
```

**ç†æƒ³vså®é™…**:
| GPUæ•°é‡ | ç†æƒ³åŠ é€Ÿæ¯” | å®é™…åŠ é€Ÿæ¯” | æ•ˆç‡ |
|---------|-----------|-----------|------|
| 1       | 1.0x      | 1.0x      | 100% |
| 2       | 2.0x      | 1.8x      | 90%  |
| 4       | 4.0x      | 3.4x      | 85%  |
| 8       | 8.0x      | 6.2x      | 78%  |

**æ•ˆç‡ä¸‹é™åŸå› **:
- é€šä¿¡å¼€é”€å æ¯”å¢åŠ 
- è´Ÿè½½ä¸å‡è¡¡
- åŒæ­¥ç­‰å¾…æ—¶é—´

#### 5.3 åˆ†å¸ƒå¼è®­ç»ƒè°ƒè¯•

**å¸¸è§é—®é¢˜**:

1. **è®­ç»ƒå¡ä½**:
   ```python
   # æ·»åŠ barrierè°ƒè¯•
   if dist.get_rank() == 0:
       print("Step 1")
   dist.barrier()  # ç­‰å¾…æ‰€æœ‰GPU

   if dist.get_rank() == 0:
       print("Step 2")
   ```

2. **æ€§èƒ½ä¸æ‰©å±•**:
   ```python
   # æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åˆ†ç‰‡
   sampler = DistributedSampler(dataset)
   assert len(sampler) == len(dataset) // world_size
   ```

3. **æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±**:
   ```python
   # æ£€æŸ¥æ¯ä¸ªrankçš„æ¢¯åº¦
   for name, param in model.named_parameters():
       if param.grad is not None:
           grad_norm = param.grad.norm()
           if dist.get_rank() == 0:
               print(f"{name}: grad_norm={grad_norm}")
   ```

---

### 6. å†…å­˜ä¼˜åŒ–

#### 6.1 æ˜¾å­˜åˆ†æ

**ä½¿ç”¨torch.cuda.memory**:
```python
def print_memory_usage():
    allocated = torch.cuda.memory_allocated() / 1e9
    reserved = torch.cuda.memory_reserved() / 1e9
    print(f"Allocated: {allocated:.2f} GB")
    print(f"Reserved: {reserved:.2f} GB")

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for batch in dataloader:
    print("Before forward:")
    print_memory_usage()

    output = model(batch)

    print("After forward:")
    print_memory_usage()

    loss.backward()
    optimizer.step()

    print("After backward:")
    print_memory_usage()
```

#### 6.2 æ˜¾å­˜ä¼˜åŒ–æŠ€å·§

1. **åŠæ—¶æ¸…ç†ä¸­é—´å˜é‡**:
   ```python
   # ä¸å¥½çš„åšæ³•
   for layer in layers:
       x1 = layer(x)
       x2 = process(x1)
       x3 = another_process(x2)

   # å¥½çš„åšæ³•
   for layer in layers:
       x = layer(x)
       x = process(x)
       del x  # æ˜¾å¼åˆ é™¤
       torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
   ```

2. **ä½¿ç”¨inplaceæ“ä½œ**:
   ```python
   # ä¸èŠ‚çœæ˜¾å­˜
   x = x + y

   # èŠ‚çœæ˜¾å­˜
   x += y  # inplace

   # æˆ–ä½¿ç”¨relu_
   x = torch.relu(x)    # ä¸èŠ‚çœ
   torch.relu_(x)       # èŠ‚çœ
   ```

3. **å‡å°batch size + æ¢¯åº¦ç´¯ç§¯**:
   ```python
   effective_batch = 512
   micro_batch = 32
   accumulation = effective_batch // micro_batch

   for i in range(0, len(dataloader), accumulation):
       for j in range(accumulation):
           loss = model(batch) / accumulation
           loss.backward()
       optimizer.step()
   ```

---

## ğŸ”§ å®æˆ˜ä¼˜åŒ–æ¡ˆä¾‹

### æ¡ˆä¾‹1: å›¾åƒåˆ†ç±»è®­ç»ƒä¼˜åŒ–

**åˆå§‹çŠ¶æ€**:
```
Batch size: 32
å•epochæ—¶é—´: 120s
GPUåˆ©ç”¨ç‡: 60%
```

**ä¼˜åŒ–æ­¥éª¤**:

1. **DataLoaderä¼˜åŒ–** (+30%é€Ÿåº¦):
   ```python
   num_workers: 0 â†’ 8
   pin_memory: False â†’ True
   prefetch_factor: 2 â†’ 4
   ```

2. **æ··åˆç²¾åº¦** (+40%é€Ÿåº¦):
   ```python
   with autocast(dtype=torch.bfloat16):
   ```

3. **å¢å¤§batch size** (+20%é€Ÿåº¦):
   ```python
   batch_size: 32 â†’ 64
   ```

**æœ€ç»ˆç»“æœ**:
```
Batch size: 64
å•epochæ—¶é—´: 45s (2.7xåŠ é€Ÿ)
GPUåˆ©ç”¨ç‡: 92%
```

---

### æ¡ˆä¾‹2: Transformerè®­ç»ƒä¼˜åŒ–

**åˆå§‹çŠ¶æ€**:
```
æ¨¡å‹: 1Bå‚æ•°
Batch size: 8 (å•GPU)
å•stepæ—¶é—´: 2.5s
OOMé—®é¢˜: ç»å¸¸
```

**ä¼˜åŒ–æ­¥éª¤**:

1. **æ¢¯åº¦æ£€æŸ¥ç‚¹** (-40%æ˜¾å­˜):
   ```python
   model = checkpoint_sequential(model, segments=4)
   ```

2. **FSDP** (-70%æ˜¾å­˜):
   ```python
   model = FSDP(model, sharding_strategy=ShardingStrategy.FULL_SHARD)
   ```

3. **DDPæ‰©å±•** (4 GPU):
   ```python
   model = DDP(model)
   ```

**æœ€ç»ˆç»“æœ**:
```
æœ‰æ•ˆbatch size: 8 Ã— 4 = 32
å•stepæ—¶é—´: 0.8s (3.1xåŠ é€Ÿ)
OOMé—®é¢˜: è§£å†³
```

---

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–æ¸…å•

### è®­ç»ƒå‰æ£€æŸ¥

- [ ] ä½¿ç”¨PyTorchæœ€æ–°ç‰ˆæœ¬
- [ ] ç¡®è®¤CUDA/cuDNNç‰ˆæœ¬åŒ¹é…
- [ ] å¯ç”¨cudnn.benchmarkï¼ˆå›ºå®šè¾“å…¥å°ºå¯¸ï¼‰
  ```python
  torch.backends.cudnn.benchmark = True
  ```

### æ•°æ®åŠ è½½

- [ ] num_workers > 0ï¼ˆæ¨è4-8ï¼‰
- [ ] pin_memory=True
- [ ] prefetch_factor=2-4
- [ ] persistent_workers=Trueï¼ˆå¤§æ•°æ®é›†ï¼‰

### è®¡ç®—ä¼˜åŒ–

- [ ] ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆBF16 > FP16ï¼‰
- [ ] å¯ç”¨cudnn.benchmark
- [ ] æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶ï¼‰
- [ ] æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batchï¼‰

### åˆ†å¸ƒå¼è®­ç»ƒ

- [ ] ä½¿ç”¨DDPè€ŒéDP
- [ ] find_unused_parameters=False
- [ ] è°ƒæ•´bucket_cap_mb
- [ ] ä½¿ç”¨DistributedSampler

### å†…å­˜ç®¡ç†

- [ ] åŠæ—¶åˆ é™¤ä¸éœ€è¦çš„tensor
- [ ] ä½¿ç”¨inplaceæ“ä½œ
- [ ] å®šæœŸè°ƒç”¨torch.cuda.empty_cache()
- [ ] ç›‘æ§æ˜¾å­˜ä½¿ç”¨

---

## ğŸ¯ å­¦ä¹ æ£€éªŒ

### å…³é”®é—®é¢˜

1. **æ€§èƒ½åˆ†æ**:
   - å¦‚ä½•ä½¿ç”¨PyTorch Profilerï¼Ÿ
   - å¦‚ä½•è¯†åˆ«è®­ç»ƒç“¶é¢ˆï¼Ÿ
   - cuda_time_totalå’Œself cuda timeæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

2. **æ•°æ®åŠ è½½**:
   - num_workerså¦‚ä½•é€‰æ‹©ï¼Ÿ
   - pin_memoryçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
   - å¦‚ä½•ä¼˜åŒ–è‡ªå®šä¹‰collate_fnï¼Ÿ

3. **è®¡ç®—ä¼˜åŒ–**:
   - æ··åˆç²¾åº¦è®­ç»ƒçš„åŸç†å’Œæ³¨æ„äº‹é¡¹ï¼Ÿ
   - æ¢¯åº¦æ£€æŸ¥ç‚¹ä½•æ—¶ä½¿ç”¨ï¼Ÿ
   - ç®—å­èåˆå¦‚ä½•å®ç°ï¼Ÿ

4. **åˆ†å¸ƒå¼ä¼˜åŒ–**:
   - å¦‚ä½•åˆ†æDDPæ€§èƒ½ç“¶é¢ˆï¼Ÿ
   - å¦‚ä½•æé«˜å¤šGPUæ‰©å±•æ•ˆç‡ï¼Ÿ
   - å¦‚ä½•è°ƒè¯•åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜ï¼Ÿ

### ä»£ç ç»ƒä¹ 

å®Œæˆ [examples.py](examples.py) ä¸­çš„ç»ƒä¹ é¢˜ã€‚

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**æ–‡æ¡£**:
- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [PyTorch Profiler Tutorial](https://pytorch.org/tutorials/intermediate/profiler_tutorial.html)

**ä»£ç **:
- [PyTorch Benchmark](https://github.com/pytorch/pytorch/tree/master/benchmarks)
- [NVIDIA Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples)

---

## âš ï¸ å¸¸è§é™·é˜±

1. **è¿‡åº¦ä¼˜åŒ–**:
   - âŒ ç›²ç›®è¿½æ±‚é«˜num_workersï¼ˆå¯èƒ½å¯¼è‡´CPUç«äº‰ï¼‰
   - âœ… ç”¨ProfileréªŒè¯ä¼˜åŒ–æ•ˆæœ

2. **è¿‡æ—©ä¼˜åŒ–**:
   - âŒ åœ¨æ¨¡å‹æ²¡è°ƒé€šå‰å°±ä¼˜åŒ–æ€§èƒ½
   - âœ… å…ˆç¡®ä¿æ­£ç¡®æ€§ï¼Œå†ä¼˜åŒ–æ€§èƒ½

3. **å¿½ç•¥ç¡¬ä»¶å·®å¼‚**:
   - âŒ ä¸åŒGPUä½¿ç”¨ç›¸åŒé…ç½®
   - âœ… æ ¹æ®ç¡¬ä»¶ç‰¹æ€§è°ƒæ•´å‚æ•°

4. **åªçœ‹é€Ÿåº¦ä¸çœ‹ç²¾åº¦**:
   - âŒ æ··åˆç²¾åº¦å¯¼è‡´ç²¾åº¦ä¸‹é™
   - âœ… ç›‘æ§è®­ç»ƒæŒ‡æ ‡ï¼Œç¡®ä¿ç²¾åº¦æ— æŸ

---

**ä¸‹ä¸€æ­¥**: [Day 22-23: å‘é‡æ•°æ®åº“ä¸RAGåŸºç¡€](../Day22-23_Vector_DB_RAG/README.md)
