# Day 17-18: Flash Attentionä¸åˆ†å¸ƒå¼è®­ç»ƒ

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡Flash Attentionå®ç°ï¼Œç†è§£DDP/FSDPåŸç†ï¼Œå®ŒæˆCS336 Assignment 2æ ¸å¿ƒå†…å®¹
> **æ—¶é—´åˆ†é…**: 6å°æ—¶ï¼ˆç†è®º2h + å®è·µ4hï¼‰
> **éš¾åº¦**: â­â­â­â­â­
> **æ¥æº**: CS336 Assignment 2 - Systems
> **é‡è¦æ€§**: â­â­â­â­â­ (æ ¸å¿ƒæŠ€èƒ½ï¼Œé¢è¯•é«˜é¢‘)

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. Flash Attentionï¼šè§£å†³å†…å­˜ç“¶é¢ˆ

#### æ ‡å‡†Attentionçš„é—®é¢˜

**è®¡ç®—å¤æ‚åº¦**: O(NÂ²d)
**å†…å­˜å¤æ‚åº¦**: O(NÂ²)  â† ç“¶é¢ˆï¼

```python
# æ ‡å‡†Attentionä¼ªä»£ç 
def standard_attention(Q, K, V):
    # Q, K, V: [batch, n_heads, seq_len, d]

    S = Q @ K.T / sqrt(d)  # [batch, n_heads, seq_len, seq_len] - å·¨å¤§çš„çŸ©é˜µ!
    P = softmax(S)          # åŒæ ·å¤§å°çš„çŸ©é˜µ
    O = P @ V               # [batch, n_heads, seq_len, d]

    return O
```

**é—®é¢˜**:
- åºåˆ—é•¿åº¦N=4096æ—¶ï¼Œattention matrixéœ€è¦ 4096Ã—4096Ã—4bytes = 64MBï¼ˆæ¯ä¸ªheadï¼‰
- 32ä¸ªheads = 2GB GPUå†…å­˜
- æ¢¯åº¦è¿˜éœ€è¦é¢å¤–å†…å­˜ï¼

#### Flash Attentionæ ¸å¿ƒæ€æƒ³

**Tilingï¼ˆåˆ†å—è®¡ç®—ï¼‰**:
1. å°†Q, K, Våˆ†æˆå°å—ï¼ˆblocksï¼‰
2. é€å—è®¡ç®—attentionï¼Œåªä¿ç•™å¿…è¦ä¿¡æ¯
3. é¿å…materializeå®Œæ•´çš„NÃ—NçŸ©é˜µ

**Online Softmax**:
```
å¢é‡æ›´æ–°softmaxç»Ÿè®¡é‡:
- m: å½“å‰æœ€å¤§å€¼
- l: å½“å‰å½’ä¸€åŒ–å› å­

æ–°blockåˆ°æ¥æ—¶:
m_new = max(m_old, m_block)
l_new = l_old * exp(m_old - m_new) + l_block
O_new = (O_old * l_old * exp(m_old - m_new) + O_block) / l_new
```

**ä¼˜åŠ¿**:
- âœ… å†…å­˜: O(NÂ²) â†’ O(N)
- âœ… é€Ÿåº¦: 2-4xåŠ é€Ÿï¼ˆHBMè®¿é—®ä¼˜åŒ–ï¼‰
- âœ… ç²¾ç¡®: ä¸æ ‡å‡†attentionå®Œå…¨ä¸€è‡´ï¼ˆæ•°å­¦ç­‰ä»·ï¼‰

---

### 2. åˆ†å¸ƒå¼è®­ç»ƒèŒƒå¼

#### 2.1 æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰

**æ ¸å¿ƒæ€æƒ³**: æ¯ä¸ªGPUæŒæœ‰å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬ï¼Œå¤„ç†ä¸åŒçš„æ•°æ®

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU 0  â”‚  â”‚  GPU 1  â”‚  â”‚  GPU 2  â”‚  â”‚  GPU 3  â”‚
â”‚ Model   â”‚  â”‚ Model   â”‚  â”‚ Model   â”‚  â”‚ Model   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
            AllReduce Gradient
            (æ¢¯åº¦åŒæ­¥)
```

#### 2.2 PyTorch DDP (DistributedDataParallel)

**ç‰¹æ€§**:
- âœ… é«˜æ•ˆçš„æ¢¯åº¦åŒæ­¥ï¼ˆAllReduceï¼‰
- âœ… æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹è¿è¡Œ
- âœ… æ”¯æŒå¤šæœºå¤šå¡
- âœ… è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯

**å®ç°**:
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–è¿›ç¨‹ç»„
dist.init_process_group(backend="nccl")

# åŒ…è£…æ¨¡å‹
model = DDP(model, device_ids=[local_rank])

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    loss = model(batch)
    loss.backward()  # è‡ªåŠ¨åŒæ­¥æ¢¯åº¦
    optimizer.step()
```

#### 2.3 FSDP (Fully Sharded Data Parallel)

**æ ¸å¿ƒæ€æƒ³**: åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€

```
æ ‡å‡†DDP:          FSDP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0   â”‚       â”‚ GPU 0   â”‚
â”‚ Model   â”‚       â”‚ Layer 1 â”‚
â”‚ (å®Œæ•´)  â”‚       â”‚  (1/4)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 1   â”‚       â”‚ GPU 1   â”‚
â”‚ Model   â”‚  â†’    â”‚ Layer 2 â”‚
â”‚ (å®Œæ•´)  â”‚       â”‚  (1/4)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜åŠ¿**:
- âœ… å†…å­˜èŠ‚çœ: å¯è®­ç»ƒè¶…å¤§æ¨¡å‹
- âœ… é€šä¿¡ä¼˜åŒ–: å‡å°‘é€šä¿¡é‡
- âœ… çµæ´»æ€§: å¯é…ç½®åˆ†ç‰‡ç²’åº¦

---

## ğŸ”§ CS336 Assignment 2 è¦æ±‚è¯¦è§£

### Part 1: Flash Attentionå®ç°ï¼ˆ3å°æ—¶ï¼‰

**æ–‡ä»¶**: `cs336_systems/attention.py`

#### ä»»åŠ¡1.1: PyTorchå®ç°ï¼ˆå¿…åšï¼Œ1.5hï¼‰

**è¦æ±‚**: å®ç°Flash Attentionçš„autogradå‡½æ•°

```python
class FlashAttentionFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, q, k, v, is_causal):
        """
        Args:
            q: [batch, n_heads, seq_len, d]
            k: [batch, n_heads, seq_len, d]
            v: [batch, n_heads, seq_len, d]
            is_causal: bool

        Returns:
            output: [batch, n_heads, seq_len, d]
        """
        # TODO: å®ç°å‰å‘ä¼ æ’­
        # 1. åˆ†å—å¤„ç†Q, K, V
        # 2. è®¡ç®—online softmax
        # 3. è¿”å›è¾“å‡ºå’ŒLï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰

        raise NotImplementedError

    @staticmethod
    def backward(ctx, do):
        """
        Args:
            do: [batch, n_heads, seq_len, d]  (è¾“å‡ºæ¢¯åº¦)

        Returns:
            dq, dk, dv: è¾“å…¥æ¢¯åº¦
        """
        # TODO: å®ç°åå‘ä¼ æ’­
        # 1. ä½¿ç”¨ä¿å­˜çš„Lé‡æ–°è®¡ç®—attention
        # 2. è®¡ç®—dS, dP
        # 3. åˆ†å—è®¡ç®—dq, dk, dv

        raise NotImplementedError
```

**æµ‹è¯•**:
```bash
uv run pytest tests/test_attention.py::test_flash_forward_pass_pytorch -v
uv run pytest tests/test_attention.py::test_flash_backward_pytorch -v
```

#### ä»»åŠ¡1.2: Tritonå®ç°ï¼ˆå¯é€‰ï¼Œ1.5hï¼‰

**è¦æ±‚**: ä½¿ç”¨Tritonç¼–å†™GPU kernel

```python
import triton

@triton.jit
def flash_attention_kernel(
    q_ptr, k_ptr, v_ptr, o_ptr,
    stride_q, stride_k, stride_v, stride_o,
    seq_len, d,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    Triton kernel for flash attention
    """
    # TODO: å®ç°Triton kernel
    pass
```

**ä¼˜åŠ¿**:
- æ¯”PyTorchå®ç°å¿«2-3x
- æ‰‹åŠ¨ä¼˜åŒ–å†…å­˜è®¿é—®

**æµ‹è¯•**:
```bash
uv run pytest tests/test_attention.py::test_flash_forward_pass_triton -v
```

---

### Part 2: DDPå®ç°ï¼ˆ2å°æ—¶ï¼‰

**æ–‡ä»¶**: `cs336_systems/parallel.py`

#### ä»»åŠ¡2.1: åŸºç¡€DDPè®­ç»ƒï¼ˆ1hï¼‰

**è¦æ±‚**: ç¼–å†™å¤šGPUè®­ç»ƒè„šæœ¬

```python
def train_with_ddp(rank, world_size):
    """
    ä½¿ç”¨DDPè®­ç»ƒæ¨¡å‹

    Args:
        rank: å½“å‰è¿›ç¨‹rank
        world_size: æ€»è¿›ç¨‹æ•°
    """
    # 1. åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend="nccl",
        rank=rank,
        world_size=world_size
    )

    # 2. è®¾ç½®device
    torch.cuda.set_device(rank)

    # 3. åˆ›å»ºæ¨¡å‹å¹¶åŒ…è£…DDP
    model = create_model().cuda(rank)
    model = DDP(model, device_ids=[rank])

    # 4. è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        for batch in dataloader:
            # ... è®­ç»ƒä»£ç 

    # 5. æ¸…ç†
    dist.destroy_process_group()
```

**æµ‹è¯•**:
```bash
# å•æœºå¤šå¡
torchrun --nproc_per_node=4 train.py

# å¤šæœºå¤šå¡
torchrun --nnodes=2 --nproc_per_node=4 train.py
```

#### ä»»åŠ¡2.2: æ¢¯åº¦ç´¯ç§¯ä¸DDPï¼ˆ0.5hï¼‰

**è¦æ±‚**: åœ¨DDPä¸­å®ç°æ¢¯åº¦ç´¯ç§¯

```python
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**æ³¨æ„**:
- âœ… DDPè‡ªåŠ¨åŒæ­¥æ¢¯åº¦
- âœ… æ¢¯åº¦ç´¯ç§¯åœ¨DDPä¹‹ä¸Š

---

### Part 3: FSDPå®è·µï¼ˆå¯é€‰ï¼Œ1å°æ—¶ï¼‰

**æ–‡ä»¶**: `cs336_systems/parallel.py`

**è¦æ±‚**: ä½¿ç”¨FSDPè®­ç»ƒå¤§æ¨¡å‹

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

def train_with_fsdp():
    # é…ç½®FSDP
    model = FSDP(
        create_model(),
        sharding_strategy="FULL_SHARD",  # å®Œå…¨åˆ†ç‰‡
        cpu_offload=CPUOffload(offload_params=True),  # CPU offload
    )

    # è®­ç»ƒå¾ªç¯ä¸DDPç›¸åŒ
    for batch in dataloader:
        loss = model(batch)
        loss.backward()
        optimizer.step()
```

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. Flash Attentionå‰å‘ä¼ æ’­

```python
def flash_attention_forward(q, k, v, is_causal, block_size=64):
    """
    Flash Attentionå‰å‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰
    """
    batch, n_heads, seq_len, d = q.shape

    # åˆå§‹åŒ–è¾“å‡ºå’Œç»Ÿè®¡é‡
    o = torch.zeros_like(q)
    l = torch.zeros(batch, n_heads, seq_len, device=q.device)
    m = torch.full((batch, n_heads, seq_len), -float('inf'), device=q.device)

    # åˆ†å—å¤„ç†
    for start_j in range(0, seq_len, block_size):
        end_j = min(start_j + block_size, seq_len)

        # åŠ è½½K, Vå—
        k_block = k[:, :, start_j:end_j, :]
        v_block = v[:, :, start_j:end_j, :]

        for start_i in range(0, seq_len, block_size):
            end_i = min(start_i + block_size, seq_len)

            # åŠ è½½Qå—
            q_block = q[:, :, start_i:end_i, :]

            # è®¡ç®—attention scores
            s_block = torch.einsum('bhqd,bhkd->bhqk', q_block, k_block) / math.sqrt(d)

            # åº”ç”¨causal mask
            if is_causal:
                mask = torch.arange(start_i, end_i, device=q.device)[:, None] >= \
                       torch.arange(start_j, end_j, device=q.device)[None, :]
                s_block = s_block.masked_fill(~mask, -float('inf'))

            # æ›´æ–°ç»Ÿè®¡é‡
            m_new = torch.maximum(m[:, :, start_i:end_i], s_block.max(dim=-1).values)
            l_new = torch.exp(m[:, :, start_i:end_i] - m_new).unsqueeze(-1) * \
                    l[:, :, start_i:end_i].unsqueeze(-1) + \
                    torch.exp(s_block - m_new.unsqueeze(-1)).sum(dim=-1)

            # æ›´æ–°è¾“å‡º
            o[:, :, start_i:end_i, :] = (
                torch.exp(m[:, :, start_i:end_i].unsqueeze(-1) - m_new.unsqueeze(-1)) *
                o[:, :, start_i:end_i, :] * l[:, :, start_i:end_i].unsqueeze(-1) +
                torch.einsum('bhqk,bhkd->bhqd', torch.exp(s_block - m_new.unsqueeze(-1)), v_block)
            ) / l_new.unsqueeze(-1)

            # æ›´æ–°ç»Ÿè®¡é‡
            m[:, :, start_i:end_i] = m_new
            l[:, :, start_i:end_i] = l_new

    return o, l
```

### 2. DDPåˆå§‹åŒ–æœ€ä½³å®è·µ

```python
import os
import torch.distributed as dist

def setup_ddp():
    """DDPç¯å¢ƒè®¾ç½®"""
    # ä»ç¯å¢ƒå˜é‡è·å–åˆ†å¸ƒå¼ä¿¡æ¯
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    local_rank = int(os.environ['LOCAL_RANK'])

    # è®¾ç½®device
    torch.cuda.set_device(local_rank)

    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',  # GPUä½¿ç”¨ncclï¼ŒCPUå¯ç”¨gloo
        rank=rank,
        world_size=world_size
    )

    return rank, world_size, local_rank

def cleanup_ddp():
    """æ¸…ç†DDPç¯å¢ƒ"""
    dist.destroy_process_group()
```

### 3. æ€§èƒ½ç›‘æ§

```python
import time

def benchmark_ddp(model, dataloader, epochs=3):
    """DDPè®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    rank = dist.get_rank()

    times = []
    for epoch in range(epochs):
        epoch_start = time.time()

        for batch in dataloader:
            loss = model(batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        epoch_time = time.time() - epoch_start
        times.append(epoch_time)

        if rank == 0:
            print(f"Epoch {epoch}: {epoch_time:.2f}s")

    avg_time = sum(times) / len(times)
    if rank == 0:
        print(f"å¹³å‡æ¯epoch: {avg_time:.2f}s")

    return avg_time
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### Flash Attention vs Standard Attention

| åºåˆ—é•¿åº¦ | æ ‡å‡† Attention | Flash Attention | åŠ é€Ÿæ¯” | å†…å­˜èŠ‚çœ |
|---------|---------------|----------------|--------|----------|
| 1024    | 100ms         | 45ms           | 2.2x   | 50%      |
| 2048    | 450ms         | 120ms          | 3.8x   | 65%      |
| 4096    | 1800ms        | 380ms          | 4.7x   | 75%      |
| 8192    | OOM           | 1500ms         | âˆ      | 80%      |

### DDP vs å•GPU

| GPUæ•°é‡ | å•GPUæ—¶é—´ | DDPæ—¶é—´ | åŠ é€Ÿæ¯” | æ•ˆç‡ |
|---------|----------|---------|--------|------|
| 1       | 100s     | 100s    | 1.0x   | 100% |
| 2       | 100s     | 52s     | 1.9x   | 95%  |
| 4       | 100s     | 28s     | 3.6x   | 90%  |
| 8       | 100s     | 15s     | 6.7x   | 84%  |

### FSDP vs DDP

| æ¨¡å‹å¤§å° | DDPå†…å­˜ | FSDPå†…å­˜ | å†…å­˜èŠ‚çœ |
|---------|---------|----------|----------|
| 1B      | 4GB     | 2GB      | 50%      |
| 10B     | 40GB    | 8GB      | 80%      |
| 100B    | OOM     | 32GB     | >90%     |

---

## ğŸ¯ å­¦ä¹ æ£€éªŒ

### å…³é”®é—®é¢˜

1. **Flash Attention**:
   - ä¸ºä»€ä¹ˆéœ€è¦Online Softmaxï¼Ÿ
   - Tilingå¦‚ä½•å‡å°‘HBMè®¿é—®ï¼Ÿ
   - åå‘ä¼ æ’­å¦‚ä½•é«˜æ•ˆè®¡ç®—ï¼Ÿ

2. **DDP**:
   - AllReduceå¦‚ä½•åŒæ­¥æ¢¯åº¦ï¼Ÿ
   - Gradient Bucketingæ˜¯ä»€ä¹ˆï¼Ÿ
   - å¦‚ä½•å¤„ç†ä¸åŒæ­¥çš„batch sizeï¼Ÿ

3. **FSDP**:
   - ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ç”¨FSDPè€Œä¸æ˜¯DDPï¼Ÿ
   - CPU Offloadå¦‚ä½•å·¥ä½œï¼Ÿ
   - Sharding Strategyå¦‚ä½•é€‰æ‹©ï¼Ÿ

### ä»£ç ç»ƒä¹ 

å®Œæˆ [examples.py](examples.py) ä¸­çš„ç»ƒä¹ é¢˜ã€‚

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

**è®ºæ–‡**:
- "Flash Attention: Faster Attention with Io-Awareness" (Dao et al., 2022)
- "FlashAttention-2: Faster Attention with Better Parallelism" (Dao, 2023)
- "PyTorch Distributed: Experiences on Scaling Distributed Training" (ML team)

**ä»£ç å‚è€ƒ**:
- [PyTorch DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Triton Flash Attention](https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py)
- [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/)

---

## âš ï¸ å¸¸è§é™·é˜±

1. **Flash Attention**:
   - âŒ å¿˜è®°ä¿å­˜Lç”¨äºåå‘ä¼ æ’­
   - âŒ Causal maskå®ç°é”™è¯¯
   - âœ… ä½¿ç”¨unit testéªŒè¯æ¢¯åº¦

2. **DDP**:
   - âŒ æ²¡æœ‰æ­£ç¡®è®¾ç½®CUDA_VISIBLE_DEVICES
   - âŒ DataLoaderæ²¡æœ‰è®¾ç½®sampler
   - âœ… ä½¿ç”¨torchrunè€Œä¸æ˜¯æ‰‹åŠ¨å¯åŠ¨è¿›ç¨‹

3. **FSDP**:
   - âŒ ä¸æ”¯æŒæŸäº›æ“ä½œï¼ˆå¦‚åŠ¨æ€shapeï¼‰
   - âŒ CPU Offloadé…ç½®ä¸å½“å¯¼è‡´å˜æ…¢
   - âœ… é€æ­¥å¢å¤§æ¨¡å‹å¤§å°æµ‹è¯•

---

**ä¸‹ä¸€æ­¥**: [Day 19: ç³»ç»Ÿä¼˜åŒ–](../Day19_System_Optimization/README.md)
